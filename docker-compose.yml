# Docker Compose for Holocron Analytics local development
# Run: docker compose up --build
#
# Services:
#   - sqlserver: SQL Server Developer Edition (persisted)
#   - initdb: Creates database if missing (runs once, then exits)
#   - seed: Loads seed data into tables (runs once, then exits)
#   - ollama: Local LLM runtime for LLM-derived data (persisted models)
#
# After startup, SQL Server and Ollama remain running for interactive use.
# Connect with SSMS/Azure Data Studio: localhost,1433 | sa | <your password>
# Ollama API: http://localhost:11434

services:
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: holocron-sqlserver
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_PID=Developer
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD}
    ports:
      - "1433:1433"
    volumes:
      - mssql_data:/var/opt/mssql
    healthcheck:
      test: ["CMD-SHELL", "bash -c \"echo > /dev/tcp/localhost/1433\""]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  initdb:
    image: mcr.microsoft.com/mssql-tools
    container_name: holocron-initdb
    depends_on:
      sqlserver:
        condition: service_healthy
    environment:
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD}
      - DATABASE_NAME=${MSSQL_DATABASE:-Holocron}
    volumes:
      - ./docker/init-db.sql:/init-db.sql:ro
      - ./src/db/ddl:/ddl:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        set -e
        echo "Waiting for SQL Server to accept connections..."
        for i in {1..60}; do
          /opt/mssql-tools/bin/sqlcmd -S sqlserver -U sa -P "$${MSSQL_SA_PASSWORD}" -C -Q "SELECT 1" -b && break
          sleep 2
        done
 
        # Function to run SQL scripts from a directory
        run_scripts() {
          local dir="$$1"
          if [ -d "$${dir}" ]; then
            for script in "$${dir}"/*.sql; do
              if [ -f "$${script}" ]; then
                echo "Running $${script}..."
                /opt/mssql-tools/bin/sqlcmd -S sqlserver -U sa -P "$${MSSQL_SA_PASSWORD}" -C \
                  -d "$${DATABASE_NAME}" -i "$${script}"
              fi
            done
          fi
        }
        
        echo "=== Creating database if not exists ==="
        /opt/mssql-tools/bin/sqlcmd -S sqlserver -U sa -P "$${MSSQL_SA_PASSWORD}" -C \
          -Q "IF DB_ID(N'$${DATABASE_NAME}') IS NULL BEGIN CREATE DATABASE [$${DATABASE_NAME}]; PRINT 'Database $${DATABASE_NAME} created successfully.'; END ELSE BEGIN PRINT 'Database $${DATABASE_NAME} already exists.'; END"

        echo "=== Running DDL scripts ==="
        run_scripts "/ddl/01_dimensions"
        run_scripts "/ddl/02_facts"
        run_scripts "/ddl/03_bridges"
        
        echo "=== Database initialization complete ==="

  seed:
    build:
      context: .
      dockerfile: docker/Dockerfile.seed
    container_name: holocron-seed
    depends_on:
      initdb:
        condition: service_completed_successfully
    environment:
      - SEED_SQLSERVER_HOST=sqlserver
      - SEED_SQLSERVER_DATABASE=${MSSQL_DATABASE:-Holocron}
      - SEED_SQLSERVER_USER=sa
      - SEED_SQLSERVER_PASSWORD=${MSSQL_SA_PASSWORD}
      - SEED_SQLSERVER_PORT=1433
      - SEED_SQLSERVER_DRIVER=ODBC Driver 18 for SQL Server
      - SEED_SKIP=${SEED_SKIP:-false}
    command: >
      /bin/bash -c '
        if [ "$${SEED_SKIP}" = "true" ]; then
          echo "SEED_SKIP=true, skipping seed loader"
          exit 0
        fi
        python src/ingest/seed_loader.py --all --verbose --no-file-log
      '
    volumes:
      # Mount src read-only for development (allows code changes without rebuild)
      - ./src:/app/src:ro

  # =========================================================================
  # Ollama - Local LLM Runtime
  # =========================================================================
  # Provides local LLM inference for the LLM-Derived Data subsystem.
  # Models are persisted in a named volume across restarts.
  #
  # Usage:
  #   docker compose up -d ollama
  #   docker exec -it holocron-ollama ollama pull llama3.2
  #   curl http://localhost:11434/api/tags
  #
  # GPU support is optional. See docs/llm/ollama-docker.md for details.
  # =========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: holocron-ollama
    ports:
      # Bind to localhost only for security (no LAN exposure)
      - "127.0.0.1:11434:11434"
    volumes:
      # Persist downloaded models across container restarts/rebuilds
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # Optional: GPU support for NVIDIA GPUs (WSL2 + Docker Desktop)
    # Uncomment the following section if you have NVIDIA GPU with CUDA support.
    # This is NOT required - Ollama will work with CPU-only inference.
    # To verify GPU support: docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
    #
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  mssql_data:
    driver: local
  ollama_data:
    driver: local

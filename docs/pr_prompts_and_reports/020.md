# PR #20: Add LLM-Derived Data Phase 0 documentation and interrogations scaffolding

## Header

| Field | Value |
|---|---|
| **PR Number** | 20 |
| **Title** | Add LLM-Derived Data Phase 0 documentation and interrogations scaffolding |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/20 |
| **State** | merged |
| **Created** | 2026-01-21T22:52:37Z |
| **Updated** | 2026-01-21T23:02:09Z |
| **Merged** | 2026-01-21T23:02:09Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-llm-derived-data-subsystem` |
| **Merge commit SHA** | `8d0f1bb0d9a4bb79f4378adbbdbd9dccbd1d45ec` |

---

## PR Body

Phase 0 foundation for the LLM-Derived Data subsystem: structured extraction from evidence bundles into schema-validated JSON artifacts with full reproducibility via manifests.

## Documentation (`docs/llm/`)

- **`vision-and-roadmap.md`** — Vision, core concepts (interrogation, evidence bundle, manifest, contract-first, evidence-first), and phased roadmap (Phase 0–7)
- **`glossary.md`** — Terminology definitions for all subsystem concepts
- **`status.md`** — Living implementation checklist
- **`governance.md`** / **`lineage.md`** — Placeholders for Phase 7 (retention, redaction, traceability)

## Scaffolding (`src/llm/interrogations/`)

New package for the interrogation catalog:

```
src/llm/interrogations/
├── README.md                              # Catalog concept
├── definitions/entity_extraction.yaml    # Example definition
└── rubrics/entity_extraction_rubric.md   # Example field rubric
```

Interrogation definition format:

```yaml
id: entity_extraction_v1
schema_ref: contracts/derived_output_schema.json
rubric_ref: interrogations/rubrics/entity_extraction_rubric.md
vocabularies:
  entity_type: [person, place, organization, event, concept, other]
```

## Index Updates

- `docs/DOCS_INDEX.md` — Links to all new docs
- `src/llm/README.md` — Directory structure includes interrogations

All existing `src/llm/` scaffolding (contracts, core, providers, runners, storage, prompts, smoke test) was already in place.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> # Copilot Agent Brief — LLM-Derived Data Vision + Roadmap (Docs-Heavy, Scaffold-With-Intent)
>
> ## Audience
> GitHub Copilot Agent Mode (PR-focused). This brief defines **what** to add/update in the repo for the next iteration.  
> Emphasis: **vision, documentation, and forward-compatible scaffolding** (placeholders are allowed and expected).
>
> ---
>
> # 1) Vision (What we are building, and Why)
>
> ## What this subsystem is
> We are introducing an **LLM-Derived Data** subsystem that converts **evidence bundles** (internal docs, web snapshots, SQL result sets, raw HTTP responses, transcripts, and other harvested artifacts) into **structured, schema-bound JSON outputs** (“derived artifacts”) that are reproducible and auditable.
>
> The core goal is to produce **queryable, standardized derived information**—facts, classifications, measures, relationships, and data-quality signals—**anchored to citeable evidence**, not freeform narrative.
>
> This is not a conversational agent; each run is **autonomous** (no chat history), but is still **oriented** through:
> - consistent interrogation definitions (“prompt families”)
> - evidence bundles
> - strict JSON contracts and rubrics
> - explicit provenance and run manifests
>
> ## Why this matters
> - **Scales curation:** turns a growing corpus of harvested content into structured outputs at scale.
> - **Evidence-led, audit-ready:** outputs are treated as **claims**, with citations to evidence bundle IDs.
> - **Benchmarkable across models:** supports multi-model comparisons (quality, speed, schema validity, citation integrity).
> - **Reproducible by design:** every run stores inputs, evidence IDs, model config, prompt hash, and raw response.
>
> ## What this system produces (high-level)
> - **Atomic claims** (falsifiable statements with evidence IDs)
> - **Closed-set classifications** (labels from controlled vocabularies)
> - **Rubric-anchored measures/scores** (numeric outputs with explicit anchors)
> - **Relationship candidates** (graph edges / bridge outputs)
> - **Data-quality signals** (contradictions, ambiguity flags, low-evidence warnings)
> - **Backlog generation** (new questions/jobs to resolve gaps)
>
> ## What this system is NOT
> - Not “ground truth”; outputs are **candidate claims** until verified.
> - Not primarily “synthetic data generation” (statistical fake-record generation).  
>   These are best described as **LLM-derived structured artifacts / annotations**.
> - Not a memory-based chat system; each job is independent.
>
> ## Core concepts (must appear in docs)
> - **Interrogation:** a repeatable question pattern with a JSON contract + rubric rules.
> - **Evidence bundle:** curated evidence objects with stable IDs; model cites only these IDs.
> - **Manifest:** run metadata capturing versions, inputs, evidence refs/hashes, model config, timing, raw output refs, validation.
> - **Contract-first:** output must validate; nulls allowed with `null_reason`.
> - **Evidence-first:** claims must cite evidence IDs; unsupported fields return null.
>
> ---
>
> # 2) Roadmap (Phase 0 through maturity)
>
> > NOTE: For this PR, we are implementing Phase 0 deliverables and leaving placeholders for later phases.
>
> ## Phase 0 — Foundations and scaffolding (Docs-first)
> **Intent:** establish vocabulary, structure, contracts, and minimal runnable spine; avoid locking in details.
>
> **Deliverables:**
> - `docs/llm/` vision + glossary + policy placeholders
> - contract placeholder schemas (manifest + derived output)
> - `src/llm/` scaffolding aligned to future phases (interfaces/stubs)
> - agent guidance updates (`agents/` + root `agents.md`)
> - docs index updates
>
> **Decisions kept open (document as TBD):**
> - Native Ollama API vs OpenAI-compatible endpoints as the standard call path
> - Exact SQL Server schema / stored procedures (scaffold only)
> - Vector store + embeddings strategy
>
> ## Phase 1 — MVP runner (single interrogation, single model, deterministic logging)
> - Minimal SQL Server queue schema + atomic claim-next semantics
> - One interrogation implemented end-to-end (claims + citations)
> - Artifact persistence: manifest/evidence/raw/parsed + validation results
>
> ## Phase 2 — Evidence assembly (internal docs + SQL evidence sets)
> - Evidence bundle builders for doc chunks, SQL result artifacts, raw HTTP responses
> - Bounding/sampling rules; redaction hooks
>
> ## Phase 3 — RAG / retrieval
> - Chunking + embeddings + vector retrieval to build evidence bundles
> - Relevance/dedupe/token budgeting rules
>
> ## Phase 4 — Web evidence (deterministic snapshotting + source policy)
> - Search/fetch/snapshot/extract pipeline (model citations must map to stored snapshots)
> - Domain allow/deny policy and citation integrity checks
>
> ## Phase 5 — Multi-model benchmarking and adjudication
> - Model registry + run groups (same job across models)
> - Metrics (schema validity, citation validity, latency, disagreement)
> - Optional adjudic...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/kyledmorgan/holocron-analytics/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

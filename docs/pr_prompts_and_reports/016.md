# PR #16: Add LLM-derived data scaffolding and documentation

## Header

| Field | Value |
|---|---|
| **PR Number** | 16 |
| **Title** | Add LLM-derived data scaffolding and documentation |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/16 |
| **State** | merged |
| **Created** | 2026-01-21T22:35:14Z |
| **Updated** | 2026-01-21T22:49:34Z |
| **Merged** | 2026-01-21T22:49:34Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-repository-scaffolding` |
| **Merge commit SHA** | `283c6c8c65e777ba89575f719f621df96dba147c` |

---

## PR Body

Foundation scaffolding for an LLM-derived data subsystem that generates structured JSON artifacts from evidence bundles. Phase 0: folder structure, interfaces, and docs onlyâ€”no full implementation.

## Changes

### `src/llm/` module structure
- **`contracts/`** â€” JSON schemas for manifests and derived outputs with versioning docs
- **`core/`** â€” Dataclasses (`DeriveManifest`, `EvidenceBundle`, `LLMConfig`), custom exceptions, logging wrapper
- **`providers/`** â€” `OllamaClient` HTTP stub supporting native and OpenAI-compatible APIs
- **`runners/`** â€” `DeriveRunner` orchestration stub (load config â†’ assemble prompt â†’ call provider â†’ persist)
- **`storage/`** â€” `ArtifactStore` (filesystem lake) and `SqlQueueStore` (in-memory stub, SQL schema TBD)
- **`config/`** â€” Configuration reference and `llm.example.yaml`

### Documentation
- **`docs/llm/derived-data.md`** â€” Concepts (evidence bundles, manifests, fail-closed validation), roadmap placeholders (Phase 0â€“4+)
- **`docs/llm/ollama.md`** â€” API docs for `/api/generate`, `/api/chat`, `/v1/chat/completions`; config knobs; `OLLAMA_HOST` binding

### Agent guidance
- **`agents/llm-derived-data.md`** â€” Scope boundaries (docs-first, contract-first, evidence-only), coding conventions, guardrails
- Updated `AGENTS.md` and `agents/README.md` with pointers

### Smoke test
- **`scripts/llm_smoke_test.py`** â€” Validates Ollama connectivity, tests JSON generation, writes raw + parsed outputs

## Usage

```python
from src.llm.core.types import LLMConfig, EvidenceBundle
from src.llm.providers import OllamaClient
from src.llm.runners import DeriveRunner

config = LLMConfig(provider="ollama", model="llama3.2")
client = OllamaClient(config)
client.health_check()  # True if Ollama is reachable
```

```bash
python scripts/llm_smoke_test.py --model llama3.2
```

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> **Copilot Agent Brief â€” LLM-Derived Data Scaffolding (Docs-First, Minimal Code)**
>
> ## Mission
> Add repository scaffolding and documentation to support a new â€œLLM-derived dataâ€ subsystem that will generate **structured JSON artifacts** from evidence bundles (internal docs, web snapshots, SQL result sets, raw HTTP responses) and persist **manifests + artifacts** for reproducibility.  
> This is a **foundation pass**: prioritize folder structure, docs, and lightweight stubs over full implementation.
>
> ## Scope (What to do now)
> Focus on:
> 1) **Repository orientation + scaffolding** (where things live, how they relate)  
> 2) **Docs and standards alignment** (docs index, conventions, agent rules)  
> 3) **Minimal runnable skeleton** under `src/llm/` (interfaces + placeholders, not a full system)
>
> ## Non-Goals (Do NOT do now)
> - Do not implement the full queue engine, full RAG pipeline, or full web retrieval.
> - Do not refactor or relocate existing files broadly (no large re-org).
> - Do not â€œlock inâ€ the roadmap; only add **placeholders** for phases/next steps.
>
> ## Repository Targets
> - New code should live under: `src/llm/`
> - There will be touchpoints with:
>   - `src/ingest/` (patterns/conventions to mirror; not necessarily reused yet)
>   - database (SQL Server) for queue/metadata persistence (scaffold only)
> - Update agent guidance:
>   - `agents/` folder: add new agent instructions specific to LLM-derived data
>   - Root `agents.md`: update with a pointer to the new agent rules + any repo-wide notes
> - Update documentation:
>   - `docs/` folder: add new documentation pages and ensure the docs index references them
>
> ## Implementation Tasks (Deliverables)
>
> ### A) Create `src/llm/` scaffolding (minimal but intentional)
> Create the following structure (adjust names if the repo already has established patterns; mirror existing conventions):
>
> - `src/llm/__init__.py`
> - `src/llm/README.md`  
>   - Overview of purpose, what â€œderived dataâ€ means, and how this module relates to ingest + DB.
> - `src/llm/contracts/`
>   - `manifest_schema.json` (placeholder v1)
>   - `derived_output_schema.json` (placeholder v1)
>   - `README.md` describing schema versioning and validation expectations
> - `src/llm/core/`
>   - `types.py` (dataclasses / pydantic models *only if already used in repo*)
>   - `exceptions.py`
>   - `logging.py` (thin wrapper or reuse existing logging patterns)
> - `src/llm/prompts/`
>   - `README.md` explaining â€œinterrogation prompt familiesâ€ and JSON-contract-first prompting
>   - `templates/` (empty folder with placeholder file, do not add many prompts yet)
> - `src/llm/providers/`
>   - `ollama_client.py` (thin HTTP client stub + interface)
>   - `README.md` documenting provider strategy (native Ollama API vs OpenAI-compat)
> - `src/llm/runners/`
>   - `derive_runner.py` (stub runner: load config, assemble minimal payload, call provider, persist artifacts)
> - `src/llm/storage/`
>   - `artifact_store.py` (filesystem â€œlakeâ€ writer stub)
>   - `sql_queue_store.py` (SQL Server queue/persistence stub; no full schema required yet)
> - `src/llm/config/`
>   - `config.md` (document config options)
>   - `llm.example.yaml` (or `.env.example`, whichever fits repo conventions)
>
> Keep code minimal and compilable; prefer interfaces + docstrings over heavy logic.
>
> ### B) Document Ollama integration (research + doc, not heavy code)
> Add concise docs covering:
> - Native Ollama REST API usage:
>   - Base: `http://localhost:11434/api`
>   - Endpoints commonly used:
>     - `POST /api/generate`
>     - `POST /api/chat`
> - OpenAI-compatible endpoints (document both as options; do not choose one permanently yet):
>   - `POST /v1/chat/completions`
>   - `POST /v1/responses` (note: non-stateful flavor)
> - Configuration knobs:
>   - base URL
>   - model name
>   - stream on/off (default to off for deterministic capture in this system)
>   - timeouts and retries
> - Note operational behavior:
>   - default bind/port and how to change bind address/port (via `OLLAMA_HOST`)
>
> Put this in: `docs/llm/ollama.md` (or similar), and link it from the docs index.
>
> ### C) Add docs: â€œLLM-Derived Dataâ€ overview + placeholders for future phases
> Create `docs/llm/derived-data.md` (or similarly named) containing:
> - What this subsystem is (LLM-derived structured artifacts, evidence-led)
> - What it is not (not ground truth; not â€œsynthetic data generationâ€ in the statistical sense)
> - Concepts:
>   - evidence bundles
>   - manifests (inputs, configs, hashes)
>   - raw response vs parsed JSON
>   - schema validation (fail-closed; nulls allowed with reasons)
> - Placeholder roadmap headings ONLY (no detailed phase write-up yet):
>   - Phase 0 (foundation)
>   - Phase 1 (MVP runner)
>   - Phase 2 (evidence assembly)
>   - Phase 3 (multi-model benchmarking)
>   - Phase 4+ (hardening / lineage / governance)
>
> ### D) Update docs index + repository navigation
> - Find the existin...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

# PR #21: Add Ollama Docker Compose service for local LLM runtime

## Header

| Field | Value |
|---|---|
| **PR Number** | 21 |
| **Title** | Add Ollama Docker Compose service for local LLM runtime |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/21 |
| **State** | merged |
| **Created** | 2026-01-21T23:04:58Z |
| **Updated** | 2026-01-21T23:24:00Z |
| **Merged** | 2026-01-21T23:24:00Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-ollama-docker-support` |
| **Merge commit SHA** | `b4446b857b6d7d73310fea081f67e149f090c471` |

---

## PR Body

Adds Ollama as a Docker Compose dependency for the LLM-Derived Data subsystem. Models persist across restarts, localhost-only binding for security, GPU optional.

### Docker Compose
- `ollama` service with `ollama/ollama:latest`, port `127.0.0.1:11434:11434`
- Named volume `ollama_data` → `/root/.ollama` for model persistence
- GPU config commented (optional, documented)

### Documentation
- `docs/llm/ollama-docker.md`: Windows + WSL2 setup, GPU validation, networking (host vs container URLs), security posture
- Updated `DOCS_INDEX.md`, cross-linked related docs

### Code
- `scripts/ollama_capture_models.py`: Captures model inventory (name, digest, size, parameter size, quantization) to JSON for benchmarking prep

### Configuration
- `.env.example`: `OLLAMA_BASE_URL`, `OLLAMA_HOST_BASE_URL`, `OLLAMA_MODEL`, `OLLAMA_TIMEOUT_SECONDS`, `OLLAMA_STREAM`
- `llm.example.yaml`: Container (`http://ollama:11434`) vs host (`http://localhost:11434`) URL conventions

### Usage

```bash
docker compose up -d ollama
docker exec -it holocron-ollama ollama pull llama3.2
curl http://localhost:11434/api/tags
python scripts/ollama_capture_models.py
```

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> # Copilot Agent Brief (Verbose) — Ollama in Docker (Windows Host + WSL2 + Project Integration)
>
> ## Audience and Intent
> You are GitHub Copilot running in Agent Mode. This brief defines the work to perform in a **single PR**.  
> The goal is to make Ollama a **first-class Docker Compose dependency** of this repository so that our emerging **LLM-Derived Data** subsystem can reliably call a local endpoint, persist models, and capture model metadata for benchmarking.
>
> This iteration should remain **docs-heavy and scaffold-with-intent**. Implement only what is necessary to stand up Ollama in Docker, document it well, and add minimal code hooks that align with the future roadmap (without building the full system yet).
>
> ---
>
> # 1) Context (Why we are doing this)
>
> We are building an **LLM-Derived Data** capability that generates **structured JSON artifacts** from evidence bundles (internal docs, web snapshots, SQL result sets, raw HTTP responses, etc.) and persists:
> - a run **manifest** (provenance, config, versions, hashes),
> - the **evidence bundle** used,
> - the **raw LLM output**,
> - and the **validated parsed JSON output**.
>
> We want to run this locally, predictably, and reproducibly. Our preferred approach is **Docker-first** so that:
> - local development and testing are consistent across machines,
> - persistence (models, volumes, artifacts) is explicit,
> - migration to a Linux host later is straightforward,
> - and future scaling can separate components as needed.
>
> We are currently on a **Windows host OS**. We have **WSL2 enabled** and we will run Ollama inside Docker. This keeps our LLM runtime “in the stack,” alongside SQL Server and our code.
>
> ---
>
> # 2) Non-Goals (Strict)
>
> Do NOT implement the full derived-data runner, queue engine, RAG, or web retrieval pipeline in this PR.
>
> Specifically, do not:
> - Build a full SQL Server job queue schema or stored procedures (you may add placeholders/docs).
> - Implement multi-model orchestration or adjudication.
> - Introduce broad refactors or move existing files around.
> - Lock in major architectural decisions that the roadmap says are TBD.
>
> This PR is focused on **standing up Ollama in Docker**, documenting it, and adding **minimal provider + model-inventory capture hooks** that are safe and forward-compatible.
>
> ---
>
> # 3) Primary Goals (What success looks like after this PR)
>
> After this PR:
> 1) A developer can run: `docker compose up -d ollama` and get a working local Ollama API.
> 2) Models pulled/downloaded once remain available across restarts/rebuilds (persistent volume mounted to `/root/.ollama`).
> 3) The repository contains clear documentation for:
>    - Windows host + WSL2 backend expectations
>    - how to validate GPU passthrough (optional)
>    - how to pull/run models in the container
>    - host vs container networking conventions
> 4) The repo includes minimal code scaffolding under `src/llm/` that:
>    - can call the Ollama endpoint (thin provider client stub),
>    - can snapshot model metadata (parameter size, quantization, digest, etc.) to JSON for future benchmarking.
>
> ---
>
> # 4) Implementation Requirements (PR Deliverables)
>
> ## A) Docker Compose: add an `ollama` service (Docker-first)
>
> ### A1) Add a new Compose service: `ollama`
> - Use the official image: `ollama/ollama:latest`
> - Expose the API on port 11434, but **bind only to localhost** on the host:
>   - `127.0.0.1:11434:11434`
> - Persist model downloads using a **named Docker volume** mounted to:
>   - `/root/.ollama`
> - Restart policy should support normal workstation use:
>   - `restart: unless-stopped`
>
> ### A2) Optional GPU configuration (WSL2 path)
> - Add GPU reservation configuration in Compose using:
>   ```yaml
>   deploy:
>     resources:
>       reservations:
>         devices:
>           - driver: nvidia
>             count: all
>             capabilities: [gpu]
>   ```
> - This must be included as “optional” and should not break users without GPU support.
> - If the repo has patterns for profiles, you may add a `gpu` profile; otherwise keep it in-place but documented as optional.
>
> ### A3) Networking conventions (must document and align future code)
> We need stable endpoint conventions:
> - From the **host machine** for manual testing:
>   - `http://localhost:11434`
> - From other containers in the same Compose network:
>   - `http://ollama:11434`
>
> Ensure the Compose service is on the same network as the rest of the stack so future runner containers can call it by service name.
>
> ### A4) Minimal operational commands (document in docs)
> - Start: `docker compose up -d ollama`
> - Logs: `docker compose logs -f ollama`
> - Pull model (inside container): `docker exec -it ollama ollama pull <model>`
> - Run model (inside container): `docker exec -it ollama ollama run <model>`
> - Check inventory via API: `curl http://localhost:11434/api/tags`
>
> ---
>
> ## B) Documentation (docs-...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/kyledmorgan/holocron-analytics/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

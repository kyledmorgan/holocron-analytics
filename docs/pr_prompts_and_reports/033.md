# PR #33: Extend ingestion work items for full HTTP request/response persistence

## Header

| Field | Value |
|---|---|
| **PR Number** | 33 |
| **Title** | Extend ingestion work items for full HTTP request/response persistence |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/33 |
| **State** | merged |
| **Created** | 2026-02-06T18:11:05Z |
| **Updated** | 2026-02-06T18:24:54Z |
| **Merged** | 2026-02-06T18:24:54Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/extend-ingestion-work-items` |
| **Merge commit SHA** | `a8cbfb9c11c945e573e05ba0306158405abb5576` |

---

## PR Body

Extends the ingestion framework to persist complete HTTP request/response data and support rank-driven dual-variant (RAW + HTML) content acquisition from ranked Wikipedia resources.

### Data Model Extensions
- Added `AcquisitionVariant` enum (RAW, HTML) to `WorkItem` and `IngestRecord`
- Extended `IngestRecord` with `content_type`, `content_length`, `file_path`, `request_timestamp`, `response_timestamp`
- Updated `get_dedupe_key()` to include variant: `source:name:type:id[:variant]`

### Schema Migrations
- `0009_extended_acquisition.sql`: variant/rank columns, response metadata, indexes for source/variant/recency queries
- `0010_acquisition_views.sql`: views for hot/cold partitioning, latest successful fetch per resource+variant, resources with both variants, queue summaries by source

### Rank-Driven Queue Seeder
```python
from ingest.analysis import create_ranked_work_items, seed_ranked_queue

# Creates 2 work items per resource (RAW + HTML) from inbound link rank
work_items = create_ranked_work_items(
    source_name="wookieepedia",
    limit=50000,  # Top N by inbound link count
)

# Or directly seed the queue with deduplication
stats = seed_ranked_queue(state_store, limit=50000)
# {'total_created': 100000, 'enqueued': 100000, 'skipped_duplicates': 0}
```

### Storage & Runner Updates
- `SqlServerIngestWriter` and `FileLakeWriter` persist extended response metadata
- `IngestRunner` captures accurate request/response timestamps around HTTP calls
- Variant included in file lake filenames: `{resource}_{variant}_{timestamp}_{id}.json`

### Key Views
| View | Purpose |
|------|---------|
| `vw_recent_work_items` | Last 90 days (hot) |
| `vw_latest_successful_fetch` | Latest success per resource+variant |
| `vw_resources_with_both_variants` | Completeness check for RAW+HTML |
| `vw_queue_summary_by_source` | Source system isolation |

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# Copilot Agent Task: Extend Ingestion Work Items to Persist Full HTTP Requests + Responses (Ranked Wikipedia Pull)

## Context / Current State (Narrative)
We have completed a crawl-style ingestion pass where:
- We downloaded a very large set of resources (primarily wiki pages, but potentially also images/files and other resource types).
- We performed link analysis and produced an **inbound link rank JSON** (or equivalent artifact) that counts how many other resources link *to* each resource.
- The universe is ~1,000,000 resources; we want to avoid “a million different hits” immediately.

## Immediate Goal
Use the inbound-link ranking to drive a **targeted second-phase retrieval**:
- Start with the **top 50,000 ranked resources** (descending by inbound link count).
- For each resource, retrieve **two variants**:
  1) “raw” version (as the system defines it—could be API/raw text, or canonical “source” format)
  2) “HTML” version (rendered HTML response from HTTP)

Important: Prior runs likely created output files on disk, but we now need the database to **know exactly what was requested and what was returned**—so subsequent mining/structuring/LLM steps can be driven from the DB without re-fetching.

## Design Intent (High-Level)
We want to evolve the ingestion “work item” concept into a durable system-of-record for acquisition:
- Work items should be able to store (or reference) **the full HTTP request** and **the full HTTP response**.
- Responses become the **source-of-truth payload** for downstream mining:
  - content extraction
  - metadata extraction
  - entity/relationship mining
  - embedding / RAG preparation
  - structured dimensional/fact modeling

We do **not** want to over-prescribe exact names/paths—match to what exists. The primary requirement is: extend the existing work-item persistence model so the DB can fully represent the acquisition event and its payload(s).

## Mechanism We Need
### 1) Rank-Driven Queueing
Implement a mechanism that:
- Reads the inbound link rank artifact (JSON).
- Selects top N (initially N=50,000).
- Creates work items (or equivalent queue records) in descending rank order.
- Avoids duplicates by using stable identity keys (e.g., resource_id + variant + source_system).

### 2) Two Records Per Resource (Raw + HTML)
For each ranked resource we queue two acquisition units:
- Variant A: RAW
- Variant B: HTML

These should be trackable independently (different HTTP endpoints, different headers, different status codes, different response sizes, etc.).
They should also both tie back to the same “logical resource” identity so we can correlate/compare.

### 3) Persist Full Request + Full Response in the DB
Extend the work-item persistence layer so each acquisition record can store:
**Request**
- request_uri
- request_method
- request_headers (as text/JSON)
- request_body (if any)
- request_timestamp
- caller/correlation identifiers (optional but useful)

**Response**
- response_timestamp
- status_code
- response_headers (as text/JSON)
- response_body (full payload; potentially compressed or stored as a file reference depending on size strategy)
- content_type
- content_length (actual stored length)
- fetch_duration_ms (if available)
- error details (if failure)

We need a pragmatic approach to large payloads:
- If DB can store full payload safely (e.g., NVARCHAR(MAX)/VARBINARY(MAX)), store it.
- If payloads are large and we prefer filesystem/object storage, store:
  - content hash
  - file path/URI to the stored payload
  - plus small “snippet/preview” in DB for quick inspection.
Either path is acceptable—choose what best fits the existing architecture. The key is **DB linkage** so we can mine without re-downloading.

### 4) Link to Existing File Outputs (If They Already Exist)
If prior runs produced files (raw/html) but the DB currently lacks linkage:
- Add a reconciliation / backfill step that:
  - maps work items to file artifacts
  - writes artifact references into the DB
  - optionally stores hashes and sizes for integrity
This can be applied to the new run or as a migration pass.

## Logical Partitioning + Performance Strategy (Describe; Implement Pragmatically)
We expect growth into very large row counts (potentially hundreds of millions over time), so introduce a scalable access pattern:

### A) Partition/Segmentation by Source Type
At minimum, distinguish source/system categories (examples only):
- wikipedia / mediawiki / wookieepedia (whatever our crawler is targeting)
- openalex
- other sources in the future

Whether this is implemented as:
- a partition key,
- a separate table per source,
- or a common table + filtered indexes
…is up to the existing design. But we want the **logical capability** to isolate one source system cleanly.

### B) Recency / “Hot vs Cold” Working Set
We need to prevent day-to-day queries from scanning the entire historical corpus:
- Provide a “recent” view or partition that focuses on last ~90 days (or similar).
- Provide an “archive/ol...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/kyledmorgan/holocron-analytics/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

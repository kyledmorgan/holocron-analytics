# PR #37: Add standardized LLM page classification contract (v3_contract)

## Header

| Field | Value |
|---|---|
| **PR Number** | 37 |
| **Title** | Add standardized LLM page classification contract (v3_contract) |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/37 |
| **State** | merged |
| **Created** | 2026-02-07T05:19:17Z |
| **Updated** | 2026-02-07T05:36:24Z |
| **Merged** | 2026-02-07T05:36:24Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/fix-28459648-1134713281-046dd62a-392d-454d-9a68-1de3fc9b69ea` |
| **Merge commit SHA** | `444d8b03b9d2e81588099026cce80376577534af` |

---

## PR Body

Evolves page classification from ad-hoc prompts to a stable, model-agnostic contract. Adds structured notes field for subtype handling, clean input envelope format, and confidence calibration guidance.

### Schema (`src/llm/contracts/page_classification_v1_schema.json`)
- Constrained `primary_type` enum (12 values) with `Other` fallback
- Structured `notes` object: `likely_subtype`, `why_primary_type`, `new_type_suggestions`, `ignored_noise`
- `secondary_types[]` with `is_candidate_new_type` flag for taxonomy evolution
- `suggested_tags[]` with `tag_type` enum and `visibility` (Public/Hidden)
- `descriptor_sentence` (≤50 words, single sentence, plain text)

### Prompts (`src/llm/prompts/page_classification.py`)
- Wikipedia-style classifier role definition
- Confidence calibration: 0.90+ clear, 0.70-0.89 minor ambiguity, <0.40 very uncertain
- Noise handling instructions (ignore citations, templates, nav elements)
- Clean input envelope builder—no raw payload embedding

### Runner update
```python
from llm.prompts.page_classification import build_messages, PROMPT_VERSION

messages = build_messages(
    title="Luke Skywalker",
    namespace="Main",
    continuity_hint="Canon",
    excerpt_text=bounded_excerpt,  # from ContentExtractor
)
response = client.chat_with_structured_output(messages, output_schema)
```

### Tests
- 29 unit tests for prompts module
- All 292 existing tests pass

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> ## Copilot Agent Task: Current-state → Next-state LLM contract + prompts (model-agnostic Ollama)
>
> **Objective**
> We are evolving our LLM interrogation framework from “ad hoc classification” to a **stable, model-agnostic contract** that works the same way regardless of which Ollama model is selected. Each request is independent, so the contract must be self-contained and consistent.
>
> We want the LLM to behave like a **Wikipedia / Wookieepedia page classifier and signal extractor**:
> - Input: page identity + bounded excerpt (not full payload)
> - Output: strict JSON matching schema (no extra keys, no markdown)
> - It should choose from a baseline set of standardized categories when possible
> - If the page doesn’t fit (e.g., “droid type” or something novel), it should return a broad type and record precision/novelty in notes so we can extend pick lists later
>
> ---
>
> # Current State (what we do today)
> - We call Ollama with a minimal system message: “return only valid JSON…”
> - We pass messy payload excerpts (sometimes HTML or wikitext or raw JSON fragments)
> - We get a classification back, but:
>   - Types/tags are not consistently constrained
>   - We lack a consistent place to report “unknown/new type needed”
>   - We don’t clearly separate “standardized outputs” from “freeform notes”
>   - We don’t reliably request the short descriptor sentence (<= 50 words)
>
> ---
>
> # Next State (what we want)
> - A single, stable JSON schema used for every page classification call
> - Strong prompt instructions that:
>   - Tell the model it is a “Wikipedia page classifier”
>   - Enforce output schema strictly
>   - Encourage standardization first, novelty second
> - A clean input envelope:
>   - title, namespace, continuity_hint
>   - extracted bounded excerpt that starts at real article prose
>   - optional hints (source_system/resource_id) for traceability
>
> ---
>
> # LLM Role Definition (include verbatim in system prompt)
> You are a Wikipedia-style page classifier and metadata extractor. You will be given a page title and a bounded excerpt from the article body. Your job is to infer the page’s entity type(s), continuity hint, confidence, and suggested tags for indexing. Prefer standardized enum values; if unsure or the page represents a novel subtype (e.g., a droid class), choose the closest broad type and record subtype details in notes so the system can extend pick lists later.
>
> Output must be **ONLY** valid JSON that matches the provided schema. No markdown. No extra keys.
>
> ---
>
> # JSON Contract (output_schema) — update runner to use exactly this
> **Design intent**
> - `primary_type` is standardized and must be chosen from an enum.
> - `secondary_types` can include either:
>   - additional enum-aligned types, OR
>   - a “candidate_new_type” marker (freeform) when we need a new picklist entry.
> - `descriptor_sentence` is the short 1-sentence summary (<= 50 words).
> - `notes` captures nuance (e.g., “this is a droid model subtype: astromech”).
>
> ```json
> {
>   "type": "object",
>   "additionalProperties": false,
>   "properties": {
>     "title": { "type": "string" },
>     "namespace": {
>       "type": "string",
>       "enum": ["Main","Module","Forum","UserTalk","Wookieepedia","Other"]
>     },
>     "continuity_hint": { "type": "string", "enum": ["Canon","Legends","Unknown"] },
>
>     "primary_type": {
>       "type": "string",
>       "enum": [
>         "PersonCharacter",
>         "LocationPlace",
>         "Organization",
>         "Species",
>         "ObjectArtifact",
>         "WorkMedia",
>         "EventConflict",
>         "TimePeriod",
>         "Concept",
>         "MetaReference",
>         "TechnicalSitePage",
>         "Other"
>       ]
>     },
>
>     "secondary_types": {
>       "type": "array",
>       "items": {
>         "type": "object",
>         "additionalProperties": false,
>         "properties": {
>           "type": { "type": "string" },
>           "weight": { "type": "number" },
>           "is_candidate_new_type": { "type": "boolean" }
>         },
>         "required": ["type","weight","is_candidate_new_type"]
>       }
>     },
>
>     "descriptor_sentence": {
>       "type": "string",
>       "description": "Exactly one sentence, <= 50 words, plain text only."
>     },
>
>     "suggested_tags": {
>       "type": "array",
>       "items": {
>         "type": "object",
>         "additionalProperties": false,
>         "properties": {
>           "tag": { "type": "string" },
>           "tag_type": {
>             "type": "string",
>             "enum": ["EntityFacet","Topic","EraTime","Continuity","Affiliation","Role","Medium","Meta","Keyword"]
>           },
>           "visibility": { "type": "string", "enum": ["Public","Hidden"] },
>           "weight": { "type": "number" }
>         },
>         "required": ["tag","tag_type","visibility","weight"]
>       }
>     },
>
>     "confidence": { "type": "number" },
>     "needs_review": { "ty...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/kyledmorgan/holocron-analytics/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

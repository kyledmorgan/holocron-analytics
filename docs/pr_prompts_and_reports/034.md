# PR #34: Add concurrent runner with atomic claim/lease semantics

## Header

| Field | Value |
|---|---|
| **PR Number** | 34 |
| **Title** | Add concurrent runner with atomic claim/lease semantics |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/34 |
| **State** | merged |
| **Created** | 2026-02-06T18:26:24Z |
| **Updated** | 2026-02-06T18:49:10Z |
| **Merged** | 2026-02-06T18:49:10Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-concurrent-runners-support` |
| **Merge commit SHA** | `962b56e0e153fb60d8c00416bb1b04557e3f575b` |

---

## PR Body

Sequential ingestion is too slow for large workloads. This adds a configurable multi-worker runner with safe concurrent access to the shared work queue.

## Changes

### SQL Schema (`0011_concurrent_runner_support.sql`)
- `work_items`: added `claimed_by`, `claimed_at`, `lease_expires_at`, `last_error`, `next_retry_at`
- New `worker_heartbeats` table for liveness tracking
- Indexes for claim queries

### State Store (`sqlserver_store.py`)
- `claim_work_item()` ‚Äî atomic UPDATE with OUTPUT, handles expired lease recovery
- `renew_lease()`, `complete_work_item()`, `fail_work_item()` ‚Äî worker-validated mutations
- `recover_expired_leases()` ‚Äî reclaim stalled items
- `get_queue_stats()`, `get_active_workers()` ‚Äî observability

### Concurrent Runner (`concurrent_runner.py`)
- ThreadPoolExecutor-based N-worker orchestrator
- Configurable: `max_workers`, `lease_seconds`, backoff, rate limiting
- Graceful `shutdown()`, `pause()`, `drain()` controls
- Per-worker heartbeat updates

### File Writes (`file_lake.py`)
- Atomic write via temp file + rename
- `work_item_id` in filenames prevents collisions

### CLI
- `--concurrent`, `--max-workers`, `--stop-after`, `--source-filter`
- `--status` for queue/worker visibility

## Usage

```bash
# Run with 8 concurrent workers
python ingest_cli.py --concurrent --max-workers 8

# Check queue status
python ingest_cli.py --status
```

```yaml
# config/ingest.yaml
runner:
  max_workers: 4
  lease_seconds: 300
  base_backoff_seconds: 2.0
  max_backoff_seconds: 300.0
  respect_retry_after: true
```

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# Copilot Agent Task: Add Configurable Concurrent ‚ÄúRunners‚Äù Sharing a Single Work-Item Queue (Python)

## Context / Intent
We currently have a Python ingestion runner pattern that:
- Pulls pending work from a shared **work item queue** (SQL-backed).
- Executes HTTP fetches for target resources (e.g., wiki pages), including variants like RAW + HTML.
- Persists outputs both to:
  - **files** (JSON lake / artifacts), and
  - **SQL tables** (work item status + request/response linkage and/or content storage).
- Includes (or is expected to include) retry/backoff logic for quota / throttling responses.

Scaling goal: one sequential runner is too slow (tens of thousands ‚Üí millions of items). We want **X concurrent runner instances** operating safely against the **same shared queue**, with configurable concurrency and improved visibility (progress, backoff, rate limiting, errors).

Do NOT assume exact names of tables, folder paths, or modules‚Äîalign with what exists. Be descriptive, then implement to match the current codebase patterns.

---

## High-Level Requirements

### 1) Concurrency Model: Multiple Independent Runners, Shared Queue
Implement a configurable multitasker mode where:
- A single machine can run **N concurrent workers** (N is configurable).
- Each worker independently:
  - leases/claims a work item from the same queue,
  - executes the fetch,
  - persists results (file + SQL),
  - updates status, and
  - returns to claim more work.
- Concurrency should be adjustable without code changes (CLI args / YAML config / env vars).

Key: this is not ‚Äúagents‚Äù in an AI sense‚Äîthese are **workers/runners** processing queue items.

### 2) Correctness Under Concurrency: Atomic Claim / Lease Semantics
Prevent two workers from processing the same work item concurrently.

Add/ensure a robust **claim/lease** mechanism in the SQL queue layer:
- A worker claims an item atomically (single SQL statement/transaction), marking it ‚Äúin-progress‚Äù with:
  - `claimed_by` (worker id / hostname / pid / UUID)
  - `claimed_at`
  - `lease_expires_at` (optional but recommended)
  - `attempt_count` increment
- If a worker crashes or hangs, items can be **recovered** after `lease_expires_at` (or a watchdog reset).
- Ensure ‚Äúat-least-once‚Äù processing is acceptable; prefer idempotent persistence so replays don‚Äôt corrupt data.

Implementation detail: choose a SQL pattern appropriate to the current DB (e.g., update-with-output / select-for-update equivalent / transactional update of a TOP(1) record ordered by priority).

### 3) Priority, Batching, and Fairness
The queue likely has priority/rank (e.g., inbound link rank). Ensure:
- Workers claim items in deterministic priority order (descending rank / earliest queued / etc.).
- Optional batching: a worker can claim a small batch (k) to reduce DB contention, but MUST still be safe.
- Optional ‚Äúsource scoping‚Äù: allow workers to limit claims to a source_system (e.g., wikipedia) during focused runs.

### 4) Respectful Fetch Behavior: Backoff + Throttling + Rate Limits
We need two layers:
1) **Per-request retry/backoff** (already likely exists‚Äîreuse if present):
   - exponential backoff with jitter
   - handle HTTP 429 / 503 / 5xx
   - honor `Retry-After` header when present
2) **Global concurrency + throttling controls**:
   - cap concurrent workers (N)
   - optional per-domain or per-source rate limit (requests/sec)
   - optional circuit breaker: if throttling spikes, reduce concurrency or pause claims temporarily

Avoid hammering the target website. Make tuning parameters configurable:
- max_workers
- requests_per_second (global or per host)
- max_retries
- base_backoff_seconds, max_backoff_seconds
- respect_retry_after (true/false)

### 5) Visibility / Observability (Must-Have)
Add operational visibility suitable for ‚Äúovernight runs‚Äù and tuning:
- Per-worker logs:
  - claimed item id, resource id, variant, attempt, duration
  - status code, bytes written, persistence targets
  - backoff events (reason, delay, retry-after)
- Aggregate metrics:
  - throughput (items/min)
  - success/fail counts
  - retry counts
  - current queue depth (pending/in-progress/failed/succeeded)
  - estimated completion time (optional)
- A ‚Äúheartbeat‚Äù / status update:
  - each worker periodically writes a heartbeat row or updates `last_seen_at` so we can tell if workers are alive.
- A CLI ‚Äústatus‚Äù mode that queries the DB and prints summary:
  - pending, in-progress (by worker), succeeded (recent), failed (recent), throttled events

Store enough in SQL to support dashboarding later (even if we start with console output).

### 6) File + SQL Writes Must Be Concurrency-Safe
Ensure that multiple workers writing artifacts simultaneously won‚Äôt collide:
- Filesystem:
  - deterministic, unique file naming (work_item_id + variant + timestamp or hash)
  - atomic writes (write temp ‚Üí rename)
  - store content hash and size
- SQL:
  - idempotent upserts keyed by work_item_id / resource_id + variant + attempt (depending on sch...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ We'd love your input! Share your thoughts on Copilot coding agent in our [2 minute survey](https://gh.io/copilot-coding-agent-survey).

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

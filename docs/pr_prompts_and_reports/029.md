# PR #29: Add Bruno collection for Ollama API smoke testing

## Header

| Field | Value |
|---|---|
| **PR Number** | 29 |
| **Title** | Add Bruno collection for Ollama API smoke testing |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/29 |
| **State** | merged |
| **Created** | 2026-02-04T05:17:21Z |
| **Updated** | 2026-02-04T05:32:37Z |
| **Merged** | 2026-02-04T05:32:37Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-bruno-ollama-smoke-tests` |
| **Merge commit SHA** | `7e557bc6724623ac77004cd94edfeaf05ef6170d` |

---

## PR Body

Adds interactive Bruno collection for manual Ollama API testing. Targets Ollama-in-Docker at `http://localhost:11434` with support for container-to-container networking.

## Collection Structure

**10 requests across 5 folders:**
- **00 - Connectivity & Meta**: version, list models, list running models (`/api/version`, `/api/tags`, `/api/ps`)
- **01 - Model Lifecycle**: show details, pull model (`/api/show`, `/api/pull`)
- **02 - Generate**: plain text, JSON contract with `format: "json"` (`/api/generate`)
- **03 - Chat**: minimal, system+user with JSON contract (`/api/chat`)
- **04 - OpenAI Compatibility**: OpenAI-compatible endpoint (`/v1/chat/completions`)

All requests include Bruno tests for status codes, response structure, and JSON validation.

## Environments

- `local.bru`: Host ‚Üí Docker (`http://localhost:11434`)
- `docker-network.bru`: Container ‚Üí container (`http://ollama:11434`)

Variables: `OLLAMA_URL`, `OLLAMA_API_BASE`, `MODEL`, `STREAM`, `TEMPERATURE`, `SEED`

## JSON Contract

Structured output schema for narrative analysis:
```json
{
  "movie": { "title": "string", "year": number },
  "summary": "string",
  "key_events": [{ "event_id": "E01", "event": "string", "act": number, "confidence": number }],
  "scenes": [{ "scene_id": "S01", "setting": "string", "characters": ["string"], "confidence": number }],
  "entities": { "characters": [], "locations": [], "objects": [] },
  "open_questions": ["string"]
}
```

Contract enforces: "unknown" for uncertain fields, confidence scores (0.0-1.0), no copyrighted content reproduction.

## Data

`data/movies.top10.json`: 10 movie prompts (Empire Strikes Back, Shawshank, Inception, Matrix, etc.) with paraphrase-focused seed prompts.

## Documentation

- Collection README (468 lines): prerequisites, usage, Docker networking (OLLAMA_HOST bind address), troubleshooting, bru CLI
- AGENTS.md: Bruno `.bru` syntax, variable interpolation, environment patterns

## Usage

```bash
# Start Ollama
docker run -d -p 11434:11434 ollama/ollama

# Open collection in Bruno
tools/bruno/ollama-smoke-tests/

# Select "local" environment, run requests
```

All requests use `stream: false` for single JSON responses. Model-agnostic via `{{MODEL}}` variable.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

[!IMPORTANT]
Copilot Agent Prompt ‚Äî Add Bruno-based local LLM (Ollama-in-Docker) smoke-test harness (PR-ready)

Objective

Add a Git-friendly Bruno collection to this repo that lets us interactively smoke-test our local LLM API one request at a time (no batch runner yet). This is explicitly a manual interrogation harness: specify the model, send prompts, get JSON back, review in real time.
We‚Äôre targeting Ollama running in Docker and reachable from the host at http://localhost:11434 by default, with API under /api.

What to build (in this PR)
1) Add a Bruno collection to the repo (version-controlled)

Create a folder in-repo (pick a convention that fits this repo; default suggestion):

tools/bruno/ollama-smoke-tests/ (collection root)

Ensure it is a valid Bruno collection (include whatever collection metadata file Bruno expects in-root, e.g., bruno.json, as needed).

Add a README.md inside the collection describing:

Prereqs: Bruno installed, Ollama container running & port exposed

How to open/import the collection in Bruno UI

How to run a request and switch environments

Optional: how to run via bru CLI (nice-to-have)

2) Environments + variables (so we can switch models/hosts quickly)

Create environment files committed to git (no secrets) so devs can swap base URLs, models, and default options:

environments/local.bru (host-runner hits docker port mapping)

environments/docker-network.bru (container-to-container addressing; e.g. http://ollama:11434)

Variables to define (minimum):

OLLAMA_URL (default http://localhost:11434)

OLLAMA_API_BASE (default {{OLLAMA_URL}}/api)

MODEL (default to a commonly-available local model, but keep it easy to change)

STREAM (default false)

TEMPERATURE (default 0.2)

SEED (optional)

Notes:

Ollama binds to 127.0.0.1:11434 by default; if we need network exposure, we may set OLLAMA_HOST accordingly in docker-compose. Document this in the README.

Bruno 3.0+ stores global environments inside an environments/ folder (good for git).

3) Requests: core ‚Äúmeta‚Äù, ‚Äúsingle prompt‚Äù, ‚Äúchat‚Äù, and ‚Äústructured contracts‚Äù

Build a foldered request taxonomy in Bruno so it‚Äôs easy to browse and expand later:

Folder: 00 - Connectivity & Meta

Get version

GET {{OLLAMA_API_BASE}}/version

List models (tags)

GET {{OLLAMA_API_BASE}}/tags

List running models

GET {{OLLAMA_API_BASE}}/ps (Ollama ‚Äúrunning models‚Äù endpoint; include it per docs list)

Add basic Bruno tests for these:

status code = 200

response JSON contains expected keys (e.g., version, models[], etc.)

Folder: 01 - Model Lifecycle (optional but useful)

Show model details

POST {{OLLAMA_API_BASE}}/show with body { "model": "{{MODEL}}" }

Pull model (so we can ensure the model exists without CLI)

POST {{OLLAMA_API_BASE}}/pull with body { "model": "{{MODEL}}", "stream": false }

Folder: 02 - Generate (single prompt)

Generate: plain text

POST {{OLLAMA_API_BASE}}/generate

Body includes:

model: {{MODEL}}

prompt: {{PROMPT}} (request variable)

stream: {{STREAM}} (default false to avoid streaming in Bruno)

options: { "temperature": {{TEMPERATURE}} }

Generate: JSON contract (strict)

Same endpoint, but prompt instructs model to output ONLY JSON matching a schema (see ‚ÄúContracts‚Äù below).

Add a Bruno test that:

parses JSON successfully

asserts top-level keys exist

asserts arrays are arrays, etc.

Folder: 03 - Chat

Chat: minimal

POST {{OLLAMA_API_BASE}}/chat

Body includes:

model: {{MODEL}}

messages: [{role:"user", content:"..."}]

stream: false

Chat: system + user + contract JSON

include a system message that:

demands JSON-only output

requires ‚Äúunknown‚Äù fields rather than hallucinating specifics

Folder: 04 - OpenAI Compatibility (optional, but aligns with future runner patterns)

Add at least one request that uses Ollama‚Äôs OpenAI-compatible base:

POST {{OLLAMA_URL}}/v1/chat/completions

Minimal payload with model: {{MODEL}}, messages: [...]

Document in README that api_key is ‚Äúrequired but ignored‚Äù in typical clients; for Bruno we don‚Äôt need one.

4) ‚ÄúScenario pack‚Äù: movie-driven interrogation prompts (top-10 style)

Create a data-driven set of prompts we can run manually in Bruno to simulate how our Python runner will behave later:

Add data/movies.top10.json (committed) with objects like:

movie_title

year (optional)

seed_prompt (a safe, paraphrase-only prompt)

In Bruno, add one or more requests that reference these variables (or document how to paste one row into {{MOVIE_TITLE}} / {{PROMPT}}).

Keep prompts summary-based (no dialogue reproduction). Encourage the model to mark uncertainty and avoid ‚Äúfake precision‚Äù (e.g., don‚Äôt invent timestamps).

Example contract to implement as a prompt template (JSON-only):

{
  "movie": { "title": "...", "year": 0 },
  "summary": "1-3 paragraphs, paraphrase only",
  "key_events": [
    { "event_id": "E01", "event": "...", "act": 1, "confidence": 0.0 }
  ],
  "scenes": [
    { "scene_id": "S01", "setting": "...", "beat": "...", "characters": ["..."], "confidence": 0....

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ We'd love your input! Share your thoughts on Copilot coding agent in our [2 minute survey](https://gh.io/copilot-coding-agent-survey).

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

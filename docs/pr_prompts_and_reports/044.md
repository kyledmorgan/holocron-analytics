# PR #44: Phase 1: Create `vector` schema and runtime in parallel with `llm`

## Header

| Field | Value |
|---|---|
| **PR Number** | 44 |
| **Title** | Phase 1: Create `vector` schema and runtime in parallel with `llm` |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/44 |
| **State** | merged |
| **Created** | 2026-02-10T05:18:29Z |
| **Updated** | 2026-02-10T05:42:41Z |
| **Merged** | 2026-02-10T05:42:41Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/create-vector-runtime-parallel` |
| **Merge commit SHA** | `0279ad427d75fde504e8020f1f9b4fa09b7b61e5` |

---

## PR Body

Implements the new `vector` schema for embedding/retrieval workloads, running parallel to the `llm` chat runtime. Phase 0 established the safety baseline; this phase adds the new schema additively without touching legacy tables.

## Database

New migration `0023_create_vector_schema.sql` creates 8 tables:

- **`embedding_space`** ‚Äî First-class space identity (provider, model, dimensions). Prevents mixing incompatible vectors.
- **`job` / `run`** ‚Äî Mirrors `llm.job/run` pattern for vector operations (CHUNK_SOURCE, EMBED_CHUNKS, REEMBED_SPACE, etc.)
- **`source_registry`** ‚Äî Index state for incremental indexing
- **`chunk`** ‚Äî Canonical chunks with source linkage
- **`embedding`** ‚Äî Vectors with lineage. Idempotency via `(chunk_id, embedding_space_id, input_content_sha256)`.
- **`retrieval` / `retrieval_hit`** ‚Äî Query logging for audit/evaluation

## Python

New `src/vector/` package:

```python
from vector.store import VectorStore
from vector.contracts.models import EmbeddingSpace, VectorChunk, VectorEmbedding

store = VectorStore(connection=conn)

# Get or create embedding space
space = store.get_or_create_embedding_space(
    provider="ollama", model_name="nomic-embed-text", dimensions=768
)

# Save embedding with idempotency check
embedding = VectorEmbedding.create_new(
    chunk_id=chunk.chunk_id,
    embedding_space_id=space.embedding_space_id,
    input_content_sha256=chunk.content_sha256,
    vector=[0.1, 0.2, ...]
)
store.save_embedding(embedding)
```

Legacy `RetrievalStore` unchanged.

## Documentation

- Updated `schema-refactor-migration-notes.md` ‚Üí Phase 1 complete
- New `docs/vector/README.md`

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# Phase 1 Build-Out ‚Äî Create `vector` Runtime in Parallel (DESCRIPTIVE, NOT PRESCRIPTIVE)

> **Important for Agent Mode:**  
> This phase description is **conceptual guidance**.  
> It is intentionally **not** a literal implementation script.  
> You (the agent) should consult the Phase 0 artifacts and the current repository layout to determine:
> - exact object names, file paths, module boundaries, and migration approach
> - what is worth reusing vs re-implementing
> - what acceptance checks are most appropriate given the current codebase

---

## Current State (End of Phase 0)
Phase 0 successfully established a safe baseline for refactoring:

### Confirmed outcomes from Phase 0 artifacts
- A legacy snapshot exists:
  - `db/legacy_snapshots/llm_vector_subsystem_snapshot.sql`
  - Fully commented, non-executable DDL + rationale for legacy `llm` vector subsystem objects.

- A dependency inventory exists:
  - `docs/llm/dependency-inventory-vector-subsystem.md`
  - **No SQL stored procedures or views** currently reference legacy vector tables.
  - **Python references are isolated** to `src/llm/retrieval/` modules and are considered experimental/safe to refactor.
  - The **chat runtime** (`llm.job`, `llm.run`, `llm.artifact`, `llm.evidence_*`) has **zero** vector dependencies.

- Migration notes exist:
  - `docs/llm/schema-refactor-migration-notes.md`
  - Explicitly calls out Phase 1 as additive creation of `vector` schema and Phase 2 as cutover/deprecation.

### Implication for Phase 1
We can proceed with a parallel vector runtime confidently because:
- chat runtime is insulated (low risk of breaking changes)
- vector-related Python code appears contained and refactor-friendly
- no DB-side SQL objects are tightly coupled to the legacy vector tables

---

## Phase 1 Goal
**Introduce the new `vector` schema and runtime in parallel**, with:
- schema objects created additively (no drops yet)
- minimal but functional vector pipeline scaffolding
- optional early cutover of *experimental* vector Python modules (where it helps accelerate iteration)
- updated documentation that clearly explains the two runtimes and how they co-exist

This phase is about **standing up a working vector subsystem** (schema + basic Python wiring) without yet removing the legacy subsystem from `llm`.

---

## Phase 1 Scope (High-Level Deliverables)

### 1) Database: Create `vector` Schema Objects (Additive)
**Intent:** Establish a clean, extensible schema that can host embeddings and retrieval for ‚Äúanything embeddable.‚Äù

**Expected conceptual components**
- Vector runtime orchestration tables:
  - `vector.job` and `vector.run` (mirrored patterns from `llm.job/run`, but vector-specific semantics)

- Corpus and chunking:
  - `vector.source_registry` (index state, last indexed time, counts, tags)
  - `vector.chunk` (canonical unit of embedding + retrieval, with policy and offsets metadata)

- Embedding identity and storage:
  - `vector.embedding_space` (first-class ‚Äúvector space‚Äù identity ‚Äî must prevent accidental mixing)
  - `vector.embedding` (store vectors with lineage and idempotency)

- Retrieval logging:
  - `vector.retrieval` and `vector.retrieval_hit` (audit/evaluation; future analytics)

**Agent guidance**
- Use Phase 0 legacy snapshot as an inspiration source for which columns were useful (e.g., `source_ref_json`, `offsets_json`, `policy_json`, hashes).
- Do not assume the new schema must be a 1:1 copy. Preserve what‚Äôs valuable; improve what blocks multi-model experimentation.
- It‚Äôs acceptable to initially store vectors in JSON form for portability and ease of iteration, even if the system later adopts native vector types.
- Ensure ‚Äúspace identity‚Äù exists in the design (even if implemented in the simplest viable way at first).

---

### 2) Python: Stand Up the Vector Runtime Path (Parallel, Not Yet Mandatory)
**Intent:** Create the ability to write vector artifacts end-to-end:
- create/insert chunks
- generate embeddings using at least one provider (Ollama is fine)
- store embeddings with correct lineage in `vector.embedding`
- optionally log retrieval runs/hits

**What to inspect from Phase 0 work**
- The dependency inventory indicates vector logic currently lives in `src/llm/retrieval/` and is experimental.
- Agents should review those modules and decide:
  - what can be reused (chunk policies, query building, scoring logic)
  - what should be moved/renamed into a vector runtime namespace/module boundary

**Expected conceptual changes (flexible on exact structure)**
- Introduce a `vector`-oriented package/module area (if not already stubbed)
- Implement a repository/data-access layer for `vector.*` tables
- Implement a minimal worker/runner:
  - can be a ‚Äúdry-run‚Äù script or a queue worker, depending on what‚Äôs already in the project patterns
- Integrate with the existing backlog mindset:
  - It should be possible for vector work to be queued and processed incrementally
  - Concurrency should be feasible even if not fully enabled ...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ We'd love your input! Share your thoughts on Copilot coding agent in our [2 minute survey](https://gh.io/copilot-coding-agent-survey).

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

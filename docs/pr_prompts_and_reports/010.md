# PR #10: Add Python ingestion framework for MediaWiki/web sources with JSON data lake

## Header

| Field | Value |
|---|---|
| **PR Number** | 10 |
| **Title** | Add Python ingestion framework for MediaWiki/web sources with JSON data lake |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/10 |
| **State** | merged |
| **Created** | 2026-01-19T03:56:07Z |
| **Updated** | 2026-01-19T04:31:39Z |
| **Merged** | 2026-01-19T04:31:39Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/add-holocron-analytics-ingestion` |
| **Merge commit SHA** | `27233865155105722c2240e6208ba15ebca89081` |

---

## PR Body

Implements a pluggable Python ingestion pipeline for pulling public web content (MediaWiki API initially) into local JSON storage and SQL Server, with resumable state management and automatic link discovery.

## Architecture

**Core abstractions** (`src/ingest/core/`)
- `Connector`: Interface for data fetching (HTTP, MediaWiki)
- `StorageWriter`: Interface for persistence (file lake, SQL Server)
- `StateStore`: Interface for work queue (SQLite)
- `WorkItem`/`IngestRecord`: Data models with deduplication keys

**Implementations**
- `HttpConnector`: Generic HTTP with rate limiting, retries, exponential backoff
- `MediaWikiConnector`: Wraps MediaWiki API (`query`, `parse`, `opensearch`)
- `FileLakeWriter`: JSON files organized as `{source_system}/{source_name}/{resource_type}/`
- `SqlServerIngestWriter`: Writes to `ingest.IngestRecords` (metadata columns + JSON blob)
- `SqliteStateStore`: Work queue with status tracking (pending/in_progress/completed/failed)
- `MediaWikiDiscovery`: Extracts page links for recursive crawling

**Orchestration**
- `IngestRunner`: Main loop (dequeue â†’ fetch â†’ store â†’ discover â†’ update state)
- `IngestConfig`: YAML-based configuration loader
- `ingest_cli.py`: CLI with `--seed`, `--max-items`, `--stats` commands

## Usage

```bash
# Seed queue with Wikipedia pages
python3 src/ingest/ingest_cli.py --config config/ingest.yaml --seed

# Process 50 items with discovery enabled
python3 src/ingest/ingest_cli.py --config config/ingest.yaml --max-items 50

# Results stored in local/data_lake/mediawiki/wikipedia/page/*.json
```

## Configuration

```yaml
storage:
  data_lake:
    enabled: true
    base_dir: "local/data_lake"
  sql_server:
    enabled: false  # Optional

sources:
  - name: "wikipedia"
    type: "mediawiki"
    api_url: "https://en.wikipedia.org/w/api.php"
    rate_limit_delay: 1.0
    discovery:
      enabled: true
      discover_links: true
      max_depth: 2

seeds:
  - source: "wikipedia"
    titles: ["Star Wars", "Luke Skywalker"]
```

## State Management

SQLite tracks work items with deduplication on `{source_system}:{source_name}:{resource_type}:{resource_id}`. Crashes resume without re-fetching completed items. Status transitions: `pending` â†’ `in_progress` â†’ `completed`/`failed`.

## Storage

**File Lake**: Each response stored as `{resource_id}_{timestamp}_{ingest_id}.json` containing full metadata + payload.

**SQL Server** (optional): `ingest.IngestRecords` table with indexed metadata columns and `NVARCHAR(MAX)` JSON payload. DDL in `src/db/ddl/00_ingest/`.

## Security

- SQL identifier validation prevents injection
- Timezone-aware datetimes (Python 3.12+)
- No secrets in code (env vars/config)
- CodeQL: 0 vulnerabilities

## Future Extensions

Interfaces support adding:
- New connectors (GraphQL, RSS, scrapers)
- Discovery strategies (breadth-first, depth-first)
- Orchestration integration (Airflow/Prefect/Dagster)
- Async fetching, PySpark transforms

Dependencies: `requests`, `pyyaml`, `pyodbc` (optional)

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# Copilot Prompt â€” Holocron Analytics Ingestion Framework (Python)

You are GitHub Copilot. Help design (and optionally scaffold) a **Python-first ingestion framework** for the **Holocron Analytics** repo that can pull content from **public web sources** into a **data lake** and **SQL Server**, with an initial focus on the **MediaWiki/Wikimedia API** (e.g., Wikipedia now; Wookieepedia later).  

This is a **local-machine** run (no paid services for now), but the architecture must be **future-friendly** for scaling and orchestration later (e.g., Airflow/Prefect/Dagster, Spark/PySpark, distributed queues).

---

## 1) Current Repo Context (observed structure)

We currently have a Python project structure roughly like:

- `src/`
  - `common/`
  - `db/`
  - `ingest/`
    - `connectors/`
      - `http/`
        - `.gitkeep`
      - `mediawiki/`
        - `.gitkeep`
  - `seeds/`
  - `staging/`
- `README.md`

Copilot should propose how to evolve this structure without over-engineering.

---

## 2) Problem Statement

We need a **general ingestion pipeline** that:

1. Calls **MediaWiki API endpoints** in a structured way (index/breadth first; then drill-down/depth).
2. Supports **raw HTTP fetching / web scraping** as a secondary pattern where APIs are insufficient.
3. Tracks discovered linkages (â€œfollow the rabbit holeâ€) and avoids duplicate ingestion by maintaining a **crawl state** (what was discovered, what is pending, what is done, what failed).
4. Stores results as **JSON blobs** (raw dumps) with minimal normalized metadata.
5. Enables incremental runs and resumability on a local machine.

---

## 3) Storage Requirements (Refined)

### 3.1 JSON Ingest (Primary)
- We store the **entire response payload as a single JSON blob** (unaltered as much as practical).
- We are not extracting/transforming data into relational models yet. That comes later.

### 3.2 Minimal Metadata (Allowed / Desired)
We should store a small set of metadata fields **outside** the JSON blob to support tracking, auditing, and incremental ingestion. Examples:

- `ingest_id` (unique key; GUID/UUID recommended)
- `source_system` (e.g., `mediawiki`, `http_scrape`, `file_drop`, etc.)
- `source_name` (e.g., `wikipedia`, `wookieepedia`, or other)
- `resource_type` (e.g., `page`, `category`, `revision`, `image`, `raw_html`, etc.)
- `resource_id` (remote ID/title/URL key used for dedupe)
- `request_uri`
- `request_method`
- `request_headers` (optional; could be embedded into JSON if simpler)
- `status_code`
- `fetched_at_utc` (timestamp)
- `hash_sha256` (optional but valuable for change detection)
- `run_id` (optional grouping for a batch run)
- `attempt` / `retry_count`
- `error_message` (nullable)
- `duration_ms` (optional)

### 3.3 Storage Targets
- **Primary â€œdata lakeâ€**: files on disk (JSON files) for raw archival.
- **Secondary**: **SQL Server** storage where each ingest event is a row containing:
  - metadata columns
  - one large JSON column (e.g., `NVARCHAR(MAX)`)

We should NOT build full dimensional models at this stage.

---

## 4) Functional Scope

### 4.1 Must-Have (MVP)
- A pluggable connector interface with at least:
  - `MediaWikiConnector` (API calls)
  - `HttpConnector` (raw GET/POST for scraping or arbitrary endpoints)
- A lightweight â€œorchestrator/runnerâ€ that:
  - processes a queue of work items
  - stores results
  - updates crawl state (discovered/pending/completed/failed)
  - supports resumability (restart without losing state)
- A simple seed mechanism that:
  - starts from a set of initial targets (pages/categories/search results)
  - expands discovered targets (links/backlinks/categories) into new queue items

### 4.2 Should-Have (Near-Term)
- Rate limiting and polite crawling
- Retries with backoff
- Idempotency / dedupe strategy (donâ€™t store duplicates unnecessarily)
- Config-driven behavior (YAML/JSON/TOML)
- Logging and metrics (basic counters and run summaries)
- A â€œbreadth vs depthâ€ strategy mode

### 4.3 Nice-to-Have (Future-Friendly)
- Ability to swap local runner with an orchestrator later (Airflow/Prefect/Dagster)
- Optional parallelism (threading/async) with guardrails
- Optional Spark/PySpark integration when scale demands it

---

## 5) Non-Goals (Explicitly Out of Scope for Now)
- No paid cloud services requirement.
- No full ETL transformation into relational/star schema models yet.
- No complex UI.
- No advanced entity resolution or embeddings yet.
- No heavy distributed infrastructure required for MVP.

---

## 6) Suggested Architecture (What Copilot Should Propose)

Copilot should propose:

1. **Core abstractions**
   - `Connector` interface (`fetch(request) -> response`)
   - `WorkItem` / `IngestTask` schema
   - `Discovery` step for turning a response into new WorkItems (plugin-based)
   - `Storage` interface(s):
     - `FileLakeWriter`
     - `SqlServerIngestWriter`
   - `StateStore` interface to track queue + progress:
     - local SQLite is acceptable for MVP (recommended)
     - or SQL Server as...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

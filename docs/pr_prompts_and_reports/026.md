# PR #26: Implement Phase 3: Retrieval Augmentation (Chunking + Embeddings + Vector Store)

## Header

| Field | Value |
|---|---|
| **PR Number** | 26 |
| **Title** | Implement Phase 3: Retrieval Augmentation (Chunking + Embeddings + Vector Store) |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/26 |
| **State** | merged |
| **Created** | 2026-01-24T05:14:23Z |
| **Updated** | 2026-01-24T05:33:33Z |
| **Merged** | 2026-01-24T05:33:33Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/implement-phase-3-retrieval-augmentation` |
| **Merge commit SHA** | `7c0eb8b368074987cffb794989323600acec3f0d` |

---

## PR Body

Adds local RAG pipeline: chunk artifacts â†’ embed via Ollama â†’ store in SQL Server â†’ retrieve top-K for LLM interrogations.

## Contracts
- `ChunkRecord`, `EmbeddingRecord`, `RetrievalQuery`, `RetrievalHit` dataclasses
- Deterministic chunk IDs via SHA256(source_id + offsets + policy_version)
- Shared `compute_content_hash`, `compute_vector_hash` in `core/utils.py`

## SQL Schema (Migration 0008)
- `llm.chunk` â€” bounded text with content hash
- `llm.embedding` â€” vectors as JSON (SQL Server + Python similarity approach)
- `llm.retrieval`, `llm.retrieval_hit` â€” query audit trail
- `llm.source_registry` â€” incremental indexing support

## Retrieval Module (`src/llm/retrieval/`)
- **Chunker**: configurable size/overlap, word-boundary aware
- **Search**: cosine similarity, deterministic tie-break by chunk_id
- **Indexer CLI**: `python -m src.llm.retrieval.indexer --source-manifest sources.json --mode full|incremental`
- **Evidence converter**: retrieval hits â†’ Phase 2 `EvidenceItem`s

## Embeddings
- `OllamaClient.embed()` for `/api/embed` endpoint
- Default model: `nomic-embed-text` (configurable via `OLLAMA_EMBED_MODEL`)

## Usage

```python
# Index sources
python -m src.llm.retrieval.indexer --source-manifest sources.json --mode full

# Retrieve
from llm.retrieval.search import retrieve_chunks
result = retrieve_chunks(query_vec, candidates, "query text", "nomic-embed-text", top_k=10)

# Convert to evidence
from llm.retrieval.evidence_converter import convert_retrieval_to_evidence
items = convert_retrieval_to_evidence(result, chunk_contents)
```

## Docs
- `docs/llm/retrieval.md` â€” architecture
- `docs/llm/indexing.md` â€” how to index
- `docs/llm/operational.md` â€” retention, troubleshooting

## Tests
71 new tests covering chunking determinism, cosine similarity, retrieval ordering. 191 total passing.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> # Copilot Agent Brief (Verbose) â€” Implement Phase 3: Retrieval Augmentation (Chunking + Embeddings + Vector Store + Evidence Selection)
>
> ## Audience and Intent
> You are GitHub Copilot running in Agent Mode. This brief defines work for a **single PR** to implement **Phase 3** of the LLM-Derived Data subsystem in `holocron-analytics`.
>
> Phase 3 focus: **Retrieval Augmentation (RAG)** â€” create a local retrieval layer that:
> - chunks internal artifacts and documents into searchable units,
> - generates embeddings locally (via Ollama embeddings endpoint),
> - stores embeddings in a vector store (SQL Server using vector support where feasible, otherwise a pragmatic interim design),
> - retrieves top-K relevant chunks for a jobâ€™s interrogation,
> - builds a **bounded evidence bundle** (Phase 2) from retrieved chunks,
> - logs everything (queries, retrieval results, scoring, policies, versions) for reproducibility.
>
> This project runs locally:
> - Windows host OS
> - WSL2 + Docker Desktop
> - Ollama in Docker (LLM runtime + embeddings)
> - SQL Server in Docker (system-of-record)
> - Artifacts â€œlakeâ€ in a Docker volume
>
> This PR must implement Phase 3 end-to-end in a **minimal but real** way.
>
> ---
>
> # 1) Phase 3 Definition (What â€œDoneâ€ means)
>
> Phase 3 is complete when:
> 1) We can **index** internal artifacts into chunk records,
> 2) We can generate **embeddings** for those chunks using Ollama locally,
> 3) We can store chunk embeddings + metadata in a retrieval store,
> 4) For a new job, we can run retrieval:
>    - embed the query,
>    - search top-K chunks,
>    - assemble evidence bundle from retrieved chunks (Phase 2 builder),
> 5) We can reproduce retrieval runs using stored metadata:
>    - which embedding model, which chunker policy, which query, which scores,
> 6) We have sufficient docs and tests to operate the Phase 3 pipeline.
>
> Phase 3 does NOT include web search/snapshotting (Phase 4), and does not require multi-model adjudication (Phase 5).
>
> ---
>
> # 2) Strict Non-Goals (Do not implement in this PR)
>
> Do NOT implement:
> - Web search, crawling, or snapshotting (Phase 4)
> - Cross-machine distributed indexing
> - Complex ranking ensembles beyond simple similarity scoring + deterministic filters
> - Heavy PII redaction systems (Phase 7 hardening)
> - Broad refactors of ingest unless absolutely required
>
> Keep changes additive and aligned to existing repo patterns.
>
> ---
>
> # 3) Key Architectural Decisions (Make in this PR, document clearly)
>
> ## A) Vector store choice
> We prefer to store embeddings in SQL Server. Implement one of the following:
>
> **Option 1 (Preferred): SQL Server vector indexing**
> - If the repo environment supports it, store embeddings in SQL Server and implement similarity search in SQL.
> - Document the exact SQL Server approach used and any version/feature prerequisites.
>
> **Option 2 (Pragmatic Interim): SQL Server + Python similarity**
> - Store embeddings as VARBINARY/JSON in SQL Server, retrieve candidate sets by metadata filters, compute cosine similarity in Python, then persist retrieval results back to SQL.
> - This is acceptable for Phase 3 if SQL-native similarity is not stable/available.
>
> You must choose one option based on whatâ€™s feasible in our current Docker SQL Server.
> Regardless of option, retrieval must be deterministic and auditable.
>
> ## B) Embedding model
> Use an Ollama embedding model (configurable). Default recommendation:
> - `nomic-embed-text` (common Ollama embedding model)
>
> Embeddings must be generated locally via Ollama (no external API keys).
>
> ## C) Chunking policy
> Implement a configurable chunker:
> - chunk_size (characters or tokens estimate)
> - overlap
> - max chunks per source
> - deterministic chunk IDs
>
> Persist chunk policy parameters in SQL and in artifacts for reproducibility.
>
> ---
>
> # 4) Deliverables (Must be included in this PR)
>
> ## A) Contracts: chunk + embedding + retrieval results
>
> Under `src/llm/contracts/`, add Pydantic models for:
>
> ### A1) `ChunkRecord`
> - `chunk_id: str` (deterministic; e.g., sha256 of (source_id + offsets + policy_version))
> - `source_type: str` (lake_text, lake_http, doc, transcript, sql_result, etc.)
> - `source_ref: dict` (lake_uri, url, doc_id, etc.)
> - `offsets: dict` (byte/line range, chunk index)
> - `content: str` (bounded text stored for retrieval)
> - `content_sha256: str`
> - `byte_count: int`
> - `policy: dict` (chunk_size, overlap, version)
> - `created_utc`
>
> ### A2) `EmbeddingRecord`
> - `embedding_id: UUID`
> - `chunk_id: str` (FK-ish)
> - `embedding_model: str`
> - `vector_dim: int`
> - `vector: list[float]` (or packed representation; see SQL store)
> - `vector_sha256: str`
> - `created_utc`
>
> ### A3) `RetrievalQuery`
> - `retrieval_id: UUID`
> - `run_id: UUID` (ties to llm.run)
> - `query_text: str` (the retrieval query used)
> - `query_embedding_model: str`
> - `top_k: int`
> - `filters: dict` (source_t...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

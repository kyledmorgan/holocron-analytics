# PR #17: Implement JSON ⇄ SQL Snapshot/Replay/Sync mechanism for data lake interchange

## Header

| Field | Value |
|---|---|
| **PR Number** | 17 |
| **Title** | Implement JSON ⇄ SQL Snapshot/Replay/Sync mechanism for data lake interchange |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/17 |
| **State** | merged |
| **Created** | 2026-01-21T22:47:01Z |
| **Updated** | 2026-01-21T23:06:25Z |
| **Merged** | 2026-01-21T23:06:25Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/implement-data-interchange-layer` |
| **Merge commit SHA** | `ae659be3de2a441201cffdecbd1bc7b67614468d` |

---

## PR Body

Implements a replayable, portable data interchange layer enabling offline replay from captured JSON, bidirectional delta sync between JSON and SQL using stable content hashes, and cold storage with optional encryption.

## Core Components

- **ExchangeRecord envelope** — Universal format for HTTP/MediaWiki/OpenAlex/LLM exchanges with canonical SHA256 hashing
- **Snapshot packs** — Human-browsable NDJSON directory structure with manifest and index for fast lookups
- **SQL mirror** — `lake.RawExchangeRecord` table with hash-based deduplication and natural key conflict detection
- **Sync engine** — Bidirectional reconciliation with configurable conflict strategies (`prefer_newest`, `prefer_sql`, `prefer_json`, `fail`) and dry-run mode
- **Cold storage** — ZIP compression with optional AES-256-GCM encryption using PBKDF2 key derivation
- **Redaction hooks** — Automatic scrubbing of auth headers, tokens, and secrets

## CLI

```bash
python snapshot_cli.py init --name wookieepedia-pages --source wookieepedia --entity page --out data/snapshots/
python snapshot_cli.py sync --manifest data/snapshots/wookieepedia-pages/manifest.json --dry-run
python snapshot_cli.py pack --manifest ... --out archive.zip --encrypt
```

## SQL Schema

Creates `lake.RawExchangeRecord` with indexed `content_sha256` for O(1) delta detection and composite index on `(source_system, entity_type, natural_key)` for conflict resolution.

## Files

| Path | Purpose |
|------|---------|
| `src/ingest/snapshot/` | Core module (10 files) |
| `src/ingest/snapshot_cli.py` | CLI entry point |
| `src/db/ddl/00_ingest/003_RawExchangeRecord.sql` | Table DDL |
| `docs/data-interchange/snapshots-json-sql-sync.md` | Documentation |
| `src/ingest/tests/test_snapshot_*.py` | 42 unit tests |

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> **COPILOT AGENT MODE PROMPT — Implement JSON ⇄ SQL “Snapshot / Replay / Sync” Mechanism (Data Lake Interchange)**
>
> ## 1) Objective (What to build)
> Implement a **replayable, portable data interchange layer** so we can:
> 1) **Ingest from previously-captured JSON** (request/response envelopes) into our lake **without re-calling the web/API**.
> 2) **Export from SQL to JSON** and **import JSON to SQL** with **incremental (delta) synchronization** using stable hashes.
> 3) Support **machine-to-machine migration**, **truncate/rebuild**, and **cold storage** (optionally encrypted/compressed).
> 4) Apply to both:
>    - **External connectors** (HTTP/API/web crawler/scraper request+response)
>    - **Local generation** (LLM prompt+response + metadata), treated as first-class “exchange records”
>
> This must be implemented in a way that is **configurable**, **idempotent**, and **safe-by-default** (provenance, audit fields, redaction hooks).
>
> ## 2) Key constraints / rules
> - **Do not** require outbound network calls for “replay” mode. Replays must be deterministic from local files.
> - **Do not** break existing ingestion runners. Add functionality behind flags / new modules.
> - Prefer **additive changes**; avoid repo-wide refactors unless necessary for clean integration.
> - Use existing project patterns for:
>   - configuration (env/yaml), logging, CLI, storage, DB connectivity, and modeling.
> - Default behavior should perform a **full bidirectional reconciliation**:
>   - bring **non-duplicate unique records** into both JSON and SQL
>   - update changed records only (hash-based)
> - All hashing must use **canonical serialization** to be stable across platforms.
>
> ## 3) Start by scanning the repo (required)
> Before coding:
> 1) Identify the current ingestion flow (runner entrypoints, connectors, storage writer(s), record models).
> 2) Identify SQL Server connectivity approach (pyodbc/sqlalchemy/etc.), existing schemas/tables, and any migration mechanism (DDL scripts, Alembic, etc.).
> 3) Identify how “raw” artifacts are stored today (file lake layout, naming, metadata).
> Then implement the feature set below **using existing conventions**.
>
> ## 4) Core concept: “Exchange Record Envelope”
> Define a single envelope format for anything we ingest/export:
>
> **ExchangeRecord (logical model)**
> - `exchange_id` (uuid; always present)
> - `exchange_type` (enum): `http`, `mediawiki`, `openalex`, `llm`, `file`, etc.
> - `source_system` (string): e.g. `wookieepedia`, `openalex`, `local_llm`
> - `entity_type` (string): e.g. `page`, `work`, `completion`, etc.
> - `natural_key` (string | null): stable identifier if available (page_id, DOI, etc.)
> - `request` (object | null): request payload, URL, headers (sanitized), params, prompt, etc.
> - `response` (object | string | null): response payload (json/text), plus parse metadata
> - `observed_at_utc` (ISO timestamp)
> - `provenance` (object): runner name, host, git sha (if available), connector version
> - `content_sha256` (string): sha256 over **canonicalized** “request+response+type+source+entity+natural_key”
> - `schema_version` (int)
> - `tags` (array[string]) optional
> - `redactions_applied` (array[string]) optional
>
> Notes:
> - The envelope is the **portable unit** that can be stored as JSON and mirrored in SQL.
> - For large payloads, allow `response` to be stored as either inline JSON OR a pointer (e.g., `response_ref`) to a separate blob file. Start simple (inline), but design for extension.
>
> ## 5) File-system layout (Snapshot Packs)
> Implement a **snapshot pack** folder layout that is human-browsable:
>
> - `data/snapshots/` (or the repo’s standard data root)
>   - `<dataset_name>/`
>     - `manifest.json`
>     - `index.jsonl`  (hash + keys + file pointers; optimized for delta checks)
>     - `records/`
>       - `YYYY/`
>         - `YYYY-MM-DD/`
>           - `chunk-0001.ndjson`
>           - `chunk-0002.ndjson`
>
> Where:
> - **NDJSON** is preferred for scale (1 envelope per line).
> - `manifest.json` defines the dataset mapping + policies.
>
> **manifest.json (minimum fields)**
> - `dataset_name`
> - `description`
> - `owner`
> - `exchange_type`
> - `source_system`
> - `entity_type`
> - `sql_target`:
>   - `schema`
>   - `table`
>   - `natural_key_column` (nullable)
>   - `hash_column`
> - `sync_policy`:
>   - `direction_default`: `bidirectional` | `json_to_sql` | `sql_to_json`
>   - `conflict_strategy`: `prefer_newest` (default) | `prefer_sql` | `prefer_json` | `fail`
> - `redaction_policy` (optional)
> - `encryption_policy` (optional)
>
> ## 6) SQL mirror (Lake tables)
> Create / use a SQL structure that can store the envelope and support delta sync:
>
> Minimum SQL table (example; adapt to project conventions):
> - schema: `lake` (or existing ingestion schema)
> - table: `RawExchangeRecord`
> Columns (minimum):
> - `exchange_id` (uniqueidentifier / uuid) PK
> - `exchange_type` (varchar)
> - `source_system` (varchar)
> ...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/kyledmorgan/holocron-analytics/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

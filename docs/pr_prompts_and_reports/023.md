# PR #23: Phase 1 â€” LLM Derived Data Runner (End-to-End MVP)

## Header

| Field | Value |
|---|---|
| **PR Number** | 23 |
| **Title** | Phase 1 â€” LLM Derived Data Runner (End-to-End MVP) |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/23 |
| **State** | merged |
| **Created** | 2026-01-24T04:27:00Z |
| **Updated** | 2026-01-24T04:49:28Z |
| **Merged** | 2026-01-24T04:49:28Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/implement-derived-data-pipeline-phase-1` |
| **Merge commit SHA** | `330ee80ca72c07374e8bf0da3ca2a8f5af926d01` |

---

## PR Body

Implements the first end-to-end execution path for the LLM-Derived Data subsystem: SQL Server job queue â†’ evidence bundling â†’ Ollama structured output â†’ artifact persistence â†’ telemetry capture.

## SQL Server Schema

- `llm.job` â€” priority queue with atomic claiming via `READPAST/UPDLOCK`
- `llm.run` â€” tracks each attempt with model info and Ollama metrics
- `llm.artifact` â€” lake file references with SHA256 hashes
- Stored procedures: `usp_claim_next_job`, `usp_complete_job`, `usp_enqueue_job`

## Python Implementation

- **Contracts** (`src/llm/contracts/phase1_contracts.py`) â€” dataclasses for Job, EvidenceBundleV1, EntityFactsOutput
- **Interrogation Registry** â€” maps keys to prompt templates + JSON schemas; includes `sw_entity_facts_v1`
- **SQL Job Queue** â€” retry with exponential backoff, deadletter after max_attempts
- **Lake Writer** â€” deterministic paths: `lake/llm_runs/{yyyy}/{mm}/{dd}/{run_id}/*.json`
- **Ollama Client** â€” added `chat_with_structured_output()` passing schema in `format` field
- **Phase 1 Runner** â€” CLI with `--once` and `--loop` modes

## Docker

- Added `llm-runner` service (depends on sqlserver, ollama)
- Added `llm_lake` volume
- Updated initdb to run migrations

## Usage

```bash
# Enqueue
python scripts/llm_enqueue_job.py --entity-type character --entity-id luke \
  --evidence "Luke Skywalker was born on Tatooine in 19 BBY."

# Run
python -m src.llm.runners.phase1_runner --once --verbose

# Or via Docker
docker compose --profile llm up llm-runner
```

## Tests

56 new unit tests covering contracts, registry, and Ollama client (mocked). All 76 unit tests passing.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# PR Objective: Phase 1 â€” LLM Derived Data Runner (End-to-End MVP)

You are GitHub Copilot Agent working in the `holocron-analytics` repository.

## High-level goal
Implement Phase 1 of our â€œderived dataâ€ pipeline:
- We enqueue â€œinterrogationsâ€ (jobs) into SQL Server
- A Python runner claims jobs (priority queue semantics)
- Runner builds a controlled evidence bundle (text + metadata) per job
- Runner calls local Ollama (running in Docker) to produce *strict* structured JSON using Ollama **Structured Outputs** (JSON schema in the `format` field)
- Runner writes full request/response artifacts to our lake (filesystem volume) and writes metadata + status to SQL Server
- We can reproduce and audit every run later (payload-in, payload-out, model snapshot, timings, errors)

Phase 1 is **not** about perfect RAG, perfect truth, or web browsing. Itâ€™s about the scaffolding + first reliable end-to-end execution path.

## Constraints / Guardrails
- SQL Server is our system-of-record for queueing + run metadata (no SQLite).
- Everything must run locally on a Windows-hosted environment using Docker/WSL2.
- Keep the design extensible, but do not over-engineer. Ship the MVP.
- Capture artifacts + telemetry aggressively; correctness improvements come later.
- Avoid introducing breaking refactors to existing ingestion code unless necessary.

## Deliverables (must be included in this PR)

### 1) SQL Server schema for Phase 1 (queue + runs + artifacts)
Add a migration script under the repoâ€™s existing SQL migration pattern (choose the correct folder based on current repo conventions).

Create schema (name it one of these; pick what best matches repo conventions):
- `llm` (preferred) OR `derive`

Tables (minimum viable):
1. `llm.job` (or `derive.job`)
   - `job_id` (uniqueidentifier, PK)
   - `created_utc` (datetime2)
   - `status` (varchar; e.g., NEW, RUNNING, SUCCEEDED, FAILED, DEADLETTER)
   - `priority` (int; higher = sooner)
   - `interrogation_key` (nvarchar) â€” identifies which rubric/prompt/schema to use
   - `input_json` (nvarchar(max)) â€” the job payload (parameters, entity ids, etc.)
   - `evidence_ref_json` (nvarchar(max), nullable) â€” pointers to lake artifacts, URLs, doc ids, etc.
   - `model_hint` (nvarchar, nullable) â€” e.g., llama3.1, qwen2.5
   - `max_attempts` (int, default 3)
   - `attempt_count` (int, default 0)
   - `available_utc` (datetime2, default now) â€” for scheduling / retry backoff
   - `locked_by` (nvarchar, nullable)
   - `locked_utc` (datetime2, nullable)
   - `last_error` (nvarchar(max), nullable)
   - indexes supporting: `(status, priority desc, available_utc, created_utc)` and job claiming

2. `llm.run`
   - `run_id` (uniqueidentifier, PK)
   - `job_id` (FK)
   - `started_utc`, `completed_utc`
   - `status` (RUNNING, SUCCEEDED, FAILED)
   - `worker_id` (nvarchar)
   - `ollama_base_url` (nvarchar)
   - `model_name` (nvarchar)
   - `model_tag` (nvarchar, nullable)
   - `model_digest` (nvarchar, nullable) â€” from `/api/show` if available
   - `options_json` (nvarchar(max), nullable) â€” e.g., temperature, num_ctx, top_p
   - `metrics_json` (nvarchar(max), nullable) â€” capture Ollama response metrics:
     - total_duration, load_duration, prompt_eval_count, prompt_eval_duration, eval_count, eval_duration
   - `error` (nvarchar(max), nullable)

3. `llm.artifact`
   - `artifact_id` (uniqueidentifier, PK)
   - `run_id` (FK)
   - `artifact_type` (nvarchar) â€” request_json, response_json, evidence_bundle, prompt_text, etc.
   - `content_sha256` (nvarchar, nullable)
   - `byte_count` (bigint, nullable)
   - `lake_uri` (nvarchar) â€” relative path in our lake volume
   - `created_utc`

Stored procedure(s) (recommended for correctness under concurrency):
- `llm.usp_claim_next_job @worker_id`
  - Atomically selects the next available NEW/RETRY job using locking hints (READPAST/UPDLOCK) and marks it RUNNING + sets locked fields.
- `llm.usp_complete_job @job_id, @status, @error`
- `llm.usp_enqueue_job @priority, @interrogation_key, @input_json, @evidence_ref_json, @model_hint`

If repo already has a migrations approach and patterns for sprocs, follow that pattern.

### 2) Python code: `src/llm` package (or align with existing `src` conventions)
Implement these modules (names can vary slightly if repo has existing patterns, but keep responsibilities intact):

#### a) Ollama client
- `src/llm/clients/ollama_client.py`
- Must support:
  - `/api/chat` calls
  - Structured Outputs: pass JSON schema to the `format` field
  - Optional: `/api/show` to snapshot model metadata for each run
- Capture full request payload + full response JSON (not just the modelâ€™s textual field)

#### b) Contracts (schemas) and interrogations registry
- `src/llm/contracts/`:
  - Define Pydantic models for:
    - Job input envelope
    - Evidence bundle
    - Output contract(s) for at least ONE interrogation
- `src/llm/interrogations/`:
  - A registry mapping `interrogation_key` -> (prompt template, JSON schema, rubric metadata)
- Include one wo...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¬ We'd love your input! Share your thoughts on Copilot coding agent in our [2 minute survey](https://gh.io/copilot-coding-agent-survey).

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

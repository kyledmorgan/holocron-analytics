# PR #12: feat: Add inbound link ranking and content fetch for Wookieepedia

## Header

| Field | Value |
|---|---|
| **PR Number** | 12 |
| **Title** | feat: Add inbound link ranking and content fetch for Wookieepedia |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/12 |
| **State** | merged |
| **Created** | 2026-01-21T04:34:51Z |
| **Updated** | 2026-01-21T04:55:05Z |
| **Merged** | 2026-01-21T04:55:04Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/enhance-wookieepedia-link-rank` |
| **Merge commit SHA** | `7a1a26015a79af111744a09c7f8f3dbde92c4522` |

---

## PR Body

Adds two capabilities to the MediaWiki crawler: (A) compute inbound link frequency across page artifacts, and (B) fetch page content (wikitext + HTML) prioritized by link popularity.

## Changes

### Inbound Link Analyzer (`src/ingest/analysis/`)
- Scans `local/data_lake/mediawiki/wookieepedia/page/*.json` artifacts
- Builds `title â†’ page_id` map from known pages
- Counts inbound links from `links[].title` arrays (case-sensitive)
- Outputs sorted JSON to `analysis/inbound_link_rank.json`
- Titles without local artifacts included with `page_id: null`

### MediaWiki Connector Extensions
- `fetch_raw_content[_by_id]()` â€” wikitext via `action=query&prop=revisions&rvslots=main`
- `fetch_html_content[_by_id]()` â€” rendered HTML via `action=parse&prop=text`

### Content Work Item Seeder
- Generates work items for `content_raw` and `content_html` resource types
- Prioritizes by inbound link count (highest first)
- Deduplication via existing state store

## Usage

```bash
# Analyze link graph
python src/ingest/analysis_cli.py --analyze --source wookieepedia

# Seed top 100 pages for content fetch
python src/ingest/analysis_cli.py --seed-content --source wookieepedia --limit 100

# Run ingestion to fetch content
python src/ingest/ingest_cli.py --config config/ingest.yaml
```

## Output Format

```json
{
  "results": [
    {"page_id": 1234, "title": "Luke Skywalker", "inbound_link_count": 2847},
    {"page_id": null, "title": "The Force", "inbound_link_count": 1523}
  ]
}
```

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

# Feature Enhancement: Wookieepedia Inbound Link Rank + Content Fetch (RAW + HTML)

## Context / Current State
We already have a working MediaWiki (StarWars Fandom / Wookieepedia) crawler that:
- Calls the API to fetch `prop=links` for a page
- Writes JSON artifacts under:
  `local/data_lake/mediawiki/wookieepedia/page/`
- Uses deduping + batching and a runner pattern to enqueue new work items based on discovered links.
- Each saved JSON contains:
  - a canonical title for the page (e.g., `payload.query.pages[<pageid>].title`)
  - the numeric page id (the key in `payload.query.pages`, and/or `pageid`)
  - outbound link titles under `payload.query.pages[<pageid>].links[].title`

We now want to prioritize pulling down **article content** based on how many other pages link to the page (inbound link count).

---

## Objective (Two Deliverables)

### Deliverable A â€” Inbound Link Frequency Artifact
Scan all existing link JSON artifacts and produce a single consolidated file containing:
- `page_id`
- `title`
- `inbound_link_count` (how many other pagesâ€™ `links[].title` include this exact title)

Output should be sorted by `inbound_link_count DESC` (highest first).

**Important notes**
- Counting is based ONLY on *actual link objects* in `links[]`. Do NOT count plain string occurrences in other fields.
- Linkage is keyed by the **exact link title string** from `links[].title`.
- Titles appear case-sensitive in practice (e.g., same word with different capitalization can exist). Treat titles as **case-sensitive** for counting.
- We also want to populate `page_id` for each title in the final report by using the page artifacts we already have (the pageâ€™s own JSON contains its page_id/title).

**Acceptance Criteria (A)**
- A single file is written to disk (path defined below) that includes all pages we have artifacts for.
- Each row includes `page_id`, `title`, `inbound_link_count`.
- Sorted descending by inbound count.
- If a linked title is encountered that we do not yet have a local page artifact for (no known page_id), it may be omitted from the final list OR included with `page_id = null`; choose one approach and document it in code comments.

---

### Deliverable B â€” Extend Runner: Fetch Page Content (RAW/Wikitext + HTML)
Extend the existing ingestion pipeline to pull content for pages, driven by priority:
1) Use Deliverable Aâ€™s output to pick the most linked pages first.
2) Enqueue content work items (priority = 1 for this effort).
3) Fetch and store **two formats** for each page:
   - **RAW/SOURCE**: the canonical wikitext source of the page
   - **HTML**: a renderable HTML version of the page content

#### MediaWiki API: Recommended Calls
**RAW / Wikitext source**
Use `action=query` + `prop=revisions` with slot content:
- Example (by pageid):
  `https://starwars.fandom.com/api.php?action=query&format=json&pageids=<PAGEID>&prop=revisions&rvprop=content&rvslots=main&formatversion=2`

This returns wikitext in:
- `query.pages[0].revisions[0].slots.main.content` (formatversion=2 shape)

**HTML Render**
Use `action=parse` to get rendered HTML:
- Example (by pageid):
  `https://starwars.fandom.com/api.php?action=parse&format=json&pageid=<PAGEID>&prop=text&formatversion=2`

This returns HTML under:
- `parse.text` (formatversion=2 typically returns `parse.text` as a string)

(If needed, allow fallback to `action=parse&page=<TITLE>` if a pageid-based parse call fails, but pageid should be preferred.)

#### Storage Layout (New Folders)
Create a sibling folder next to `page/`:

- RAW/Wikitext content JSON artifacts:
  `local/data_lake/mediawiki/wookieepedia/content_raw/`

- HTML content JSON artifacts:
  `local/data_lake/mediawiki/wookieepedia/content_html/`

(Names can be `content/raw` + `content/html` if that matches existing folder conventions better, but must be stable and consistent.)

Each saved artifact should preserve the standard envelope fields used elsewhere:
- ingest_id
- source_system / source_name
- resource_type
- resource_id (title)
- request_uri, status_code, response_headers
- payload (full API response)
- timestamps

**Acceptance Criteria (B)**
- For a given `page_id/title`, we can produce:
  - one JSON file in `content_raw/` containing wikitext payload
  - one JSON file in `content_html/` containing HTML payload
- Fetching is prioritized by inbound link count (highest first), at least for an initial batch run.
- Work items are deduped so we do not refetch the same page content repeatedly.
- Failures are captured in the artifact envelope (status_code, error fields) without crashing the run.

---

## Out of Scope (Explicit)
- No semantic parsing, cleaning, summarization, embedding, or vectorization of the content in this change.
- No schema migrations unless required for minimal tracking/deduping.
- No changes to the existing link crawling behavior beyond minor shared utilities.
- No attempt to infer links from raw text; only use API `links[]`.

---

## Implementation Guidance (Fit to Existing Patterns...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

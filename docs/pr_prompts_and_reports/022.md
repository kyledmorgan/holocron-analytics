# PR #22: Complete Phase 0: LLM-Derived Data Foundations

## Header

| Field | Value |
|---|---|
| **PR Number** | 22 |
| **Title** | Complete Phase 0: LLM-Derived Data Foundations |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/22 |
| **State** | merged |
| **Created** | 2026-01-21T23:13:22Z |
| **Updated** | 2026-01-24T04:26:35Z |
| **Merged** | 2026-01-24T04:26:34Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/complete-phase-0-docs-scaffolding` |
| **Merge commit SHA** | `57af1bd10bcb3b885e7e8b94a28432158e9260df` |

---

## PR Body

Completes Phase 0 scaffolding for the LLM-Derived Data subsystem. Most infrastructure already existed; this PR fills remaining gaps and marks Phase 0 complete.

### Documentation
- `docs/llm/contracts.md` â€” Schema versioning and validation behavior
- Updated `ollama.md` with Docker Compose details and WSL2 GPU setup
- Updated `status.md` and `vision-and-roadmap.md` to mark Phase 0 complete

### Interrogation Catalog
- Rubric templates: `atomic_claims.md`, `closed_set_classification.md`, `anchored_score.md`
- Example definition: `example.atomic_claims.yaml`
- Controlled vocab: `vocab/entity_types.json`

### Tooling
- `src/llm/tools/capture_ollama_models.py` â€” Snapshot model inventory to JSON
- `src/llm/storage/README.md` â€” Documents SQL + Lake storage strategy

### Docker Compose
Added Ollama service:
```yaml
ollama:
  image: ollama/ollama:latest
  ports:
    - "127.0.0.1:11434:11434"  # localhost only
  volumes:
    - ollama_data:/root/.ollama  # persist models
  healthcheck:
    test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/11434'"]
```
GPU support commented but ready to enable.

### Usage
```bash
docker compose up ollama -d
docker exec -it holocron-ollama ollama pull llama3.2
python scripts/llm_smoke_test.py
```

All modules remain importable. Phase 1+ stubs in place but not implemented.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> [!IMPORTANT]
> # Copilot Agent Brief (Verbose) â€” Complete Phase 0: LLM-Derived Data Foundations (Docs + Scaffolding + Alignment)
>
> ## Audience and Intent
> You are GitHub Copilot running in Agent Mode. This brief defines what to do in a **single PR** to **complete Phase 0** of the LLM-Derived Data subsystem.
>
> We acknowledge:
> - Some scaffolding work may already exist under `src/llm/`
> - Work has begun on the **Ollama Docker Compose** dependency
>
> Your job is to **finish Phase 0** by ensuring:
> - repository structure is coherent and discoverable,
> - documentation is complete and indexed,
> - contract placeholders exist and are explained,
> - agent rules are updated,
> - minimal â€œsafeâ€ stubs exist for provider + artifact persistence + interrogation catalog skeleton,
> - Phase 0 â€œstatusâ€ is explicit and future phases are stubbed (but not implemented).
>
> The PR should remain **docs-heavy** and **scaffold-with-intent**. Avoid implementing Phase 1+ logic.
>
> ---
>
> # 1) Phase 0 Definition (What â€œDoneâ€ means)
>
> **Phase 0 = Foundations and scaffolding**:
> - Establish vocabulary, intent, and standards
> - Create minimal module boundaries and placeholder schemas
> - Create an interrogation catalog skeleton and rubric templates
> - Add minimal provider + storage stubs (importable, non-breaking)
> - Ensure Ollama-in-Docker is documented and integrated as a dependency (if not already finished)
> - Update agent rules and docs index
> - Add a living â€œstatusâ€ page that declares Phase 0 complete and Phase 1+ planned
>
> ---
>
> # 2) Strict Non-Goals (Phase 1+ is out of scope)
>
> Do NOT implement:
> - full SQL Server job queue schema + claim-next stored procedures (document placeholders only)
> - evidence assembly pipeline (doc chunking, SQL packaging, web snapshotting)
> - vector DB / embeddings ingestion
> - multi-model orchestration or adjudication
> - web search/retrieval tooling
> - broad refactors or file moves
>
> This PR should be additive and safe.
>
> ---
>
> # 3) Primary Deliverables (This PR)
>
> ## A) Documentation: Phase 0 must be complete and discoverable
>
> ### A1) Ensure `docs/llm/` exists with the following pages
> If any already exist, **improve and align** rather than duplicate.
>
> 1) `docs/llm/vision-and-roadmap.md`
>    - Contains the high-level â€œwhat/whyâ€
>    - Contains Phase 0â€“Phase 7 roadmap headings
>    - Clearly states: â€œWe are completing Phase 0 in this PR; Phase 1+ are plannedâ€
>
> 2) `docs/llm/glossary.md`
>    - Interrogation
>    - Evidence bundle
>    - Manifest
>    - Rubric
>    - Contract-first / evidence-first
>    - Derived artifact vs ground truth
>
> 3) `docs/llm/derived-data.md` (or merge into vision doc if repo prefers fewer pages)
>    - Explicitly defines what outputs are produced (claims, labels, measures, edges, quality signals)
>    - Clarifies â€œsyntheticâ€ language: best described as â€œLLM-derived structured artifactsâ€
>
> 4) `docs/llm/ollama-docker.md`
>    - End-to-end â€œhow to run Ollama in Docker for this projectâ€
>    - Host vs container endpoint conventions
>    - Model persistence via volume
>    - Optional GPU validation steps (WSL2 path)
>
> 5) `docs/llm/contracts.md`
>    - Explains:
>      - manifest schema purpose and versioning
>      - derived output schema purpose and versioning
>      - validation expectations
>      - â€œraw response vs parsed outputâ€ separation
>
> 6) `docs/llm/status.md`
>    - Living checklist page with:
>      - Phase 0 checklist (checked off by end of PR)
>      - Phase 1+ checklist placeholders (unchecked)
>    - Must explicitly declare Phase 0 complete after this PR.
>
> 7) Placeholders (lightweight; one paragraph each):
>    - `docs/llm/governance.md` (retention, redaction/PII modes, citation policiesâ€”TBD)
>    - `docs/llm/lineage.md` (intended lineage graphâ€”TBD)
>
> ### A2) Update documentation index
> - Find the repoâ€™s docs index (`docs/README.md`, `docs/index.md`, or equivalent).
> - Add a new section â€œLLMâ€ (or â€œLLM-Derived Dataâ€) with links to the above pages.
> - Ensure ordering is logical (start with vision, then derived data, then ollama, then status).
>
> ---
>
> ## B) Repo scaffolding under `src/llm/` (aligned to docs, minimal code)
>
> ### B1) Ensure folder structure exists and is coherent
> If this already exists, ensure names and layout are aligned and that READMEs exist.
>
> Required structure (adjust only if repo has stronger conventions):
> - `src/llm/README.md` (module overview; how it relates to ingest + SQL Server + artifacts)
> - `src/llm/__init__.py`
>
> - `src/llm/contracts/`
>   - `manifest_schema.json` (placeholder v1; valid JSON Schema)
>   - `derived_output_schema.json` (placeholder v1; valid JSON Schema)
>   - `README.md` explaining schema versioning + validation behavior
>
> - `src/llm/interrogations/`
>   - `README.md` explaining interrogation catalog concept
>   - `definitions/`
>     - `example.atomic_claims.yaml` (one example interrogation definition)
>   - `rubrics/`
>     - `atomic...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._

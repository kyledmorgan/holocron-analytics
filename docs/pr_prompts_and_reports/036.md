# PR #36: Add bounded content extraction and LLM descriptor sentence for classification

## Header

| Field | Value |
|---|---|
| **PR Number** | 36 |
| **Title** | Add bounded content extraction and LLM descriptor sentence for classification |
| **URL** | https://github.com/kyledmorgan/holocron-analytics/pull/36 |
| **State** | merged |
| **Created** | 2026-02-07T05:03:13Z |
| **Updated** | 2026-02-07T05:17:51Z |
| **Merged** | 2026-02-07T05:17:51Z |
| **Base branch** | `working_20260115` |
| **Head branch** | `copilot/improve-content-extraction` |
| **Merge commit SHA** | `5f2758a2a9f0688b6757ac95607eea974d47ee7d` |

---

## PR Body

Classification pipeline was using unbounded payload excerpts and lacked a concise entity description. This adds proper content-start detection with bounded excerpts and an LLM-generated single-sentence descriptor.

## Content Extraction (`src/semantic/content_extractor.py`)

- **Dual-path extraction**: Wikitext uses `'''` triple-quote hook; HTML uses `mw-parser-output` container + first meaningful `<p>`
- **Bounding**: min 1000, default max 8000, hard cap 12000 chars (or 35% of model context)
- **Cleanup**: Strips `<ref>` tags, converts `[[Link|Text]]` ‚Üí `Text`, removes templates
- **Metadata**: Tracks format detected, strategy, offset, length, SHA256 hash

```python
extractor = ContentExtractor(ExtractionConfig(min_chars=1000))
result = extractor.extract(payload, content_type_hint="text/x-wiki")
# result.excerpt, result.content_format, result.strategy, result.excerpt_length
```

## LLM Descriptor Sentence

- Added `descriptor_sentence` to output schema (required, ‚â§50 words, plain text only)
- Enhanced system prompt with explicit constraints and examples
- Validates/truncates word count on response

## Database (Migration 0021)

**sem.PageSignals**:
- `content_format_detected`, `content_start_strategy`, `content_start_offset`
- `lead_excerpt_text`, `lead_excerpt_len`, `lead_excerpt_hash`

**sem.PageClassification**:
- `descriptor_sentence` (nvarchar(400))

## Testing

28 unit tests covering format detection, wikitext/HTML extraction, bounding, metadata tracking, and dict payload handling.

<!-- START COPILOT CODING AGENT SUFFIX -->



<!-- START COPILOT ORIGINAL PROMPT -->



<details>

<summary>Original prompt</summary>

> ## Copilot Agent Task: Improve content extraction + add LLM ‚Äúdescriptor sentence‚Äù (<= 50 words)
>
> **Context**
> - Repo: `holocron-analytics`
> - We already have SEM staging tables/views created via recent migrations (sem.SourcePage, sem.PageSignals, sem.PageClassification, tag tables, and promotion columns on dbo.DimEntity).
> - We are running **locally only** (no cloud calls). LLM calls go to **Ollama in Docker** (e.g., container `Holocron-Dash-OLama`, port `11434`).
> - This PR focuses on **two** features:
>   1) **Better ‚Äúarticle content start‚Äù extraction** from stored payloads (raw/HTML), producing a bounded excerpt for classification.
>   2) **LLM-generated single-sentence descriptor** (<= 50 words) to store as a concise description (NOT raw payload/JSON).
>
> ---
>
> ## Feature 1: Content start extraction + bounded excerpt (no paging)
>
> **Goal**
> - When we pull a page‚Äôs stored payload from `ingest.IngestRecords` (or the existing ingest pipeline abstraction), we need to derive an excerpt that starts at actual article prose, not headers/badges/navigation/table chrome.
> - We want a **reasonably large excerpt**:
>   - Minimum target: **>= 1,000 characters** of usable text.
>   - Maximum should be **bounded** (avoid consuming the entire context window).
>   - We do **not** want to page yet. Single-shot excerpt only.
>
> **Important nuance**
> - The triple single quote marker (`'''Title'''`) is a **wikitext convention**, not an HTML convention.
> - If payload is **raw wikitext**, the `'''` hook is likely present and should be used.
> - If payload is **HTML**, do **not** assume `'''` exists. Use HTML-specific hooks.
>
> **Extraction strategy (implement both paths)**
> 1) **Detect payload format** (best-effort):
>    - If payload contains `"'\"revisions\""` and `contentformat: text/x-wiki` or has obvious wikitext markers like `{{Top|`, `==Biography==`, `'''`, treat as **wikitext**.
>    - If payload contains `<div`, `<table`, `mw-parser-output`, treat as **HTML**.
>    - If ambiguous, fall back to best available extraction (see below).
>
> 2) **Wikitext path (preferred hook)**
>    - Find first occurrence of `'''` (bold subject line) and treat that as content start.
>    - Extract from that point up to the first heading `^==` (multiline), producing the ‚Äúlead/intro‚Äù.
>    - If lead < 1,000 chars after cleanup, extend into the next section until reaching max excerpt size.
>    - Light cleanup (do not attempt full template parsing):
>      - Remove `<ref ...>...</ref>` and `<ref .../>`
>      - Convert `[[A|B]]` ‚Üí `B`, `[[A]]` ‚Üí `A`
>      - Strip or reduce `{{...}}` in extracted region (best-effort, ok to be imperfect)
>
> 3) **HTML path (hook by title/heading)**
>    - We assume HTML includes a page title and/or main content wrapper.
>    - Prefer:
>      - Extract the main content from a known container if present (e.g., `mw-parser-output`).
>      - Remove tables, nav boxes, infobox blocks if they dominate the start.
>    - Find the first meaningful paragraph block after the title/infobox region.
>      - Heuristic: first `<p>` with enough text density (e.g., >= 200 characters) within `mw-parser-output`.
>    - Strip tags to text (keep a readable plain-text excerpt).
>
> 4) **Bounding and sizing**
>    - Build excerpt text with:
>      - `min_chars = 1000`
>      - `max_chars` computed from model context budget (best-effort), otherwise default e.g. **6,000‚Äì12,000 chars**.
>    - We do not want to ‚Äúfill‚Äù the context window; target something like:
>      - `max_chars = min(12000, floor(0.35 * model_context_chars_estimate))`
>    - If we cannot reliably infer model context length, use a conservative fixed max (e.g. 8,000‚Äì12,000 chars).
>
> 5) **Persist extraction artifacts**
> - Update/insert into `sem.PageSignals` (or the most appropriate SEM table) fields such as:
>   - `content_format_detected` (wikitext/html/unknown)
>   - `content_start_strategy` (triple_quote / first_paragraph / wrapper_div / fallback)
>   - `content_start_offset` (int; byte/char index where we started)
>   - `lead_excerpt_text` (the bounded excerpt used for LLM classification)
>   - `lead_excerpt_len`
> - Keep this additive-only: new columns only if needed, otherwise reuse existing ‚Äúsignals‚Äù columns.
>
> ---
>
> ## Feature 2: LLM-generated descriptor sentence (<= 50 words)
>
> **Goal**
> - The pipeline must request from the LLM a **single descriptive sentence** that summarizes what the entity/page is.
> - Constraints:
>   - One sentence only.
>   - **<= 50 words** (prefer fewer).
>   - Must be plain text (no JSON blobs, no raw payload fragments, no markup).
>   - This will be stored as the ‚Äúdescriptor sentence‚Äù used by downstream classifiers, tagging, and UI.
>
> **Implementation details**
> - Extend the existing Ollama call‚Äôs `output_schema` to include:
>   - `descriptor_sentence` (string) ‚Äî required.
> - Update the system/user prompt to strongly enforce:
>   - ‚ÄúReturn only valid JSON matching schema.‚Äù
>   - ‚Äúdescriptor_sente...

</details>



<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ We'd love your input! Share your thoughts on Copilot coding agent in our [2 minute survey](https://gh.io/copilot-coding-agent-survey).

---

## Comments

_No comments._

---

## Review Comments (inline diff)

_No inline review comments._

---

## Copilot Artifacts

_No Copilot artifact indicators detected._
